The MVGC multivariate Granger causality toolbox: A new approach to Granger-causal inferenceContents lists available at ScienceDirect
Journal of Neuroscience Methods
jou rn al h om epa ge : www.elsev ier .com
Computational Neuroscience
h i g h l i g h t s
• Matlab© Toolbox for accurate and efﬁcient calculation of Granger causality. • Calculate Granger causalities (conditional and unconditional) in both time and frequency domains. • Based on a • Avoids sep
a r t i c l
Article history: Received 5 Sep Received in re Accepted 26 O
Keywords: Granger causa Vector autoreg Time series an
1. Introdu
∗ Correspon E-mail add
dvanced VAR (vector autoregression) theory. arate “full” and “reduced” regressions.
e i n f o
tember 2013 vised form 16 October 2013 ctober 2013
lity ressive modelling alysis
a b s t r a c t
Background: Wiener–Granger causality (“G-causality”) is a statistical notion of causality applicable to time series data, whereby cause precedes, and helps predict, effect. It is deﬁned in both time and frequency domains, and allows for the conditioning out of common causal inﬂuences. Originally developed in the context of econometric theory, it has since achieved broad application in the neurosciences and beyond. Prediction in the G-causality formalism is based on VAR (vector autoregressive) modelling. New method: The MVGC Matlab© Toolbox approach to G-causal inference is based on multiple equiva- lent representations of a VAR model by (i) regression parameters, (ii) the autocovariance sequence and (iii) the cross-power spectral density of the underlying process. It features a variety of algorithms for moving between these representations, enabling selection of the most suitable algorithms with regard to computational efﬁciency and numerical accuracy. Results: In this paper we explain the theoretical basis, computational strategy and application to empirical G-causal inference of the MVGC Toolbox. We also show via numerical simulations the advantages of our Toolbox over previous methods in terms of computational accuracy and statistical inference. Comparison with existing method(s): The standard method of computing G-causality involves estimation of parameters for both a full and a nested (reduced) VAR model. The MVGC approach, by contrast, avoids explicit estimation of the reduced model, thus eliminating a source of estimation error and improving statistical power, and in addition facilitates fast and accurate estimation of the computationally awkward case of conditional G-causality in the frequency domain. Conclusions: The MVGC Toolbox implements a ﬂexible, powerful and efﬁcient approach to G-causal inference.
© 2013 Elsevier B.V. All rights reserved.
ction
Granger causality (G-causality) (Granger, 1969; 82, 1984) is an increasingly popular method for identi- l” connectivity in neural time series data (Bressler and . It can be traced conceptually to Wiener (1956) and ionalised by Granger in terms of linear autoregressive
ding author. Tel.: +44 1273 678101. resses: l.c.barnett@sussex.ac.uk (L. Barnett), a.k.seth@sussex.ac.uk
modelling of stochastic processes (Granger, 1969). G-causality is based on predictability and precedence. Put simply, a variable X is said to G-cause a variable Y if the past of X contains information that helps predict the future of Y over and above information already in the past of Y. Importantly, G-causality is a measure of directed functional connectivity in terms of providing a statistical descrip- tion of observed responses. In contrast, methods for identifying effective connectivity aim to elucidate the “the simplest possible circuit diagram explaining observed responses” (Valdes-Sosa et al., 2011; Aertsen and Preißl, 1991; Friston et al., 2003, 2013).
The MVGC Matlab Toolbox implements numerical routines for calculating multivariate Granger causality (MVGC) from time series
see front matter ©  2013 Elsevier B.V. All rights reserved. rg/10.1016/j.jneumeth.2013.10.018
GC multivariate Granger causality tool r-causal inference
rnett ∗, Anil K. Seth for Consciousness Science and School of Informatics, University of Sussex, Brighton BN1
/ locate / jneumeth
K
L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68 51
data, both unconditional and conditional, in the time and frequency domains. It supersedes and extends the GCCA (Granger Causal Con- nectivity Analysis) Toolbox (Seth, 2010). Based on advanced VAR (vector autoregressive) model theory (Hamilton, 1994; Lütkepohl, 2005), it is Improving avoids sep thus elimin computatio multivariat
The MVG ical neurosc applying G- paper (Sect application which has b is always n tions under 3.3). Thus, and straigh standing of as approach facilitate th basis of G- tions), and d toolbox. Al help system purpose—Se rationale an computatio core compu
2. G-causa
Assume cesses (“var Y does not G is independ no informa already con of Y does beyond all i Y G-causes “causality” perhaps mo Sosa et al., 2 for example for the abo this debate exclusively
Testing a statistica seem to in information ever in emp quantities known the mators, com Bossomaier based or pa estimators ling distribu frequently also yield i predictive m
is independent of the past of Y” may be reframed as “the past of Y does not help predict X beyond the degree to which X may be predicted by its own past”.
The most common operationalisation of G-causality, and the one ch the MVGC Toolbox is based, utilises VAR modelling of time data. A multivariate time series u1, u2, . . ., um, where for me t ut is a real-valued n-dimensional (column) vector with nents u1t, u2t, . . ., unt, is considered as a realisation of length discrete-time stationary1 vector stochastic process U1, U2, e “universe of variables”). A pth order vector autoregressive
for the process—a VAR(p)—takes the form2
p
=1 Ak · U t−k + εt (1)
is lued nsio nden ; th eters varia
on s the alues ent t ictab
dete that t was pari mod t ass erve ality bout odel rath s—in s, al ). N ased e, gi ted edict on in
R pr
MVG eory
usal in tances ed”—
ughou ts sca , acco e of th ngth e, and
the de
designed for computational efﬁciency and accuracy. upon standard approaches to G-causal inference, it arate full and reduced regressions (see Section 2), ating a common source of statistical inaccuracy and nal inefﬁciency. It also facilitates estimation of fully e conditional G-causality in the frequency domain. C toolbox has been designed with application to empir- ience data in mind. Several issues commonly faced in causal inference to such data are discussed later in the ion 4). However, the software is not restricted to this
domain. G-causal inference is a very general method een productively applied in many areas, though caution eeded in ensuring that the data satisfy the assump- pinning the method (see Section 2.1 and also Section while the MVGC software is designed to be intuitive tforward to use, to get the most out of it some under-
the theoretical basis of G-causal inference, as well es to its numerical computation, is recommended. To is, this paper presents the conceptual and theoretical causality (substantially extending previous presenta- etails the computational implementation of the MVGC
though not intended as a user guide—the integrated and walk-through demonstration scripts fulﬁll that ction 3 provides a helpful overview of the toolbox d design principles, Section 3.1 explains the MVGC
nal strategy while Appendices A.1 and A.2 detail the tational algorithms.
lity: theory, estimation and inference
two jointly distributed vector-valued stochastic pro- iables”) X = X1, X2, . . ., Y = Y1, Y2, . . .. We say that -cause X if and only if X , conditional on its own past,
ent of the past of Y ; intuitively, past values of Y yield tion about the current value of X beyond information tained in the past of X itself. If, conversely, the past convey information about the future of X above and nformation contained in the past of X then we say that
X . Much has been made of the fact that this notion of does not necessarily tally with more conventional (and re physically intuitive) notions (Pearl, 2009; Valdes- 011; Friston, 2011; Roebroeck et al., 2009, 2010) (note, , that the processes must be strictly non-deterministic ve deﬁnition even to make sense). We do not engage
here; throughout this paper the term “causal” is used in the Wiener–Granger sense just described. for Granger (non-)causality requires establishing—in l sense—a conditional (non-)dependency. This may vite an information-theoretic approach, since mutual
is a natural measure of statistical dependence. How- irical settings a drawback with information-theoretic
is the difﬁculty of estimation in sample and lack of oretical distributions for information-theoretic esti- plicating statistical inference (but see Barnett and
, 2013). An alternative is therefore to invoke a model- rametric approach for which conditional dependency are more efﬁcient and (preferably) have known samp- tions, thus facilitating inference. In fact, G-causality is
identiﬁed with a model-based viewpoint, which may ntuitive predictive interpretations of G-causality; for odels, the statement “ X , conditional on its own past,
on whi series each ti compo m of a . . . (th model
U t = ∑ k
Here p real-va n-dime indepe related param uals co depend model past v repres unpred
The imply cess u In com causal explici the obs G-caus tions a VAR m that a proces as VAR section VAR-b practic is selec ture pr variati
2.1. VA
1 G-ca circums “window on.
2 Thro represen matrices the valu cess of le transpos denotes
the model order, which may be inﬁnite. The n × n matrices Ak are the regression coefﬁcients, and the nal stochastic process εt the residuals, which are tly and identically distributed (iid) and serially uncor- at is, they constitute a white noise process. The
of the model are the coefﬁcients Ak and the n × n resid- nce matrix � ≡ cov(εt) which, by stationarity, does not the time t. Interpreted as a predictive model, (1) thus
value of the process at current time t in terms of its at times t − 1, . . ., t − p. The regression coefﬁcients he predictable structure of the data, the residuals the le. rmination of a VAR model (1) should not be taken to the time series data modelled by the stochastic pro-
actually generated by a linear autoregressive scheme. son to effective connectivity techniques like dynamic elling [DCM] (Friston et al., 2003), which make very umptions about the generative mechanism underlying d data (Friston et al., 2013), the VAR models underlying
are “generic”, in the sense that they make no assump- the mechanism that produced the data, beyond that a actually exists. Standard theory (Anderson, 1971) yields er general class of covariance-stationary multivariate cluding many nonlinear processes—may be modelled beit of theoretically inﬁnite order (see also the next ote that this belies a common misapprehension that G-causal analysis is restricted to linear processes. In ven empirical time-series data, a ﬁnite model order p on theoretical principles, sufﬁciently high so as to cap- able variation but not so high as to overﬁt unpredictable
the data (Section 2.4).
ocess theory
C toolbox exploits some advanced aspects of VAR pro- , which we outline here.
G-causality analysis, the VAR coefﬁcients in (1) must mmable and stable (Hamilton, 1994; Lütkepohl, 2005; 67). Square summability requires that
∑p k=1‖Ak‖2 < ∞
ference is not necessarily limited to stationary processes. Under some —notably for multi-trial data or for near-stationary data that may be the stationarity requirement may be relaxed. This is addressed later
t, bold symbols represent vector quantities and normal typeface lar quantities. Capitalised symbols represent random variables or rding to context. Thus we write U t for the random vector representing e process U1, U2, . . . at time t, u1, . . ., um for a realisation of the pro- m, uit for the ith component of ut , etc. A superscript � denotes matrix
a superscript ∗ denotes (complex) matrix conjugate-transpose. |M| terminant of the square matrix M.
52 L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68
which means intuitively that the coefﬁcients do not “blow up” even for inﬁnite model order p (for ﬁnite p square summability is sat- isﬁed trivially). Stability means that the VAR coefﬁcients deﬁne a covariance-stationary process. This is related to the the characteristic polynomial for the coefﬁcients sequence Ak, which is:
ϕA(z) ≡ ∣∣∣∣∣I −
�(A) ≡ ma ϕA(z)
an equival covariance-
�(A) < 1
The auto necessarily of n × n mat �k ≡ cov(ut Note that b that �−k = � be shown [A ance seque being just implies tha decay (sub- as a VAR (se neural data
For a VA to the VAR 1971)
�k = p∑
�=1 A�
The cros sided) Four
�k = 1 2�
Numericall Fast Fourie (IFFT) respe
3 For simpl quency range implementatio sampling frequ the MVGC com
For a VAR process, the CPSD admits a unique spectral factorisation (Papoulis and Pillai, 2002)
S(�) = H(�)�H(�)∗ 0 ≤ � ≤ 2� (9) where the transfer function H(�) is deﬁned as the inverse matrix of the Fourier transform of the regression coefﬁcients:
H(�) ≡ (
I −
there of S(� iven ay be
the V ix inv rthe D be ke, 1 ost a
S (λ
for A mid
abilit v, 19 . (6)– ce se mati nd a s we these l pat ncy d
ncond
is op
e prec plex p that (9
able z deﬁned in the complex plane C. By standard the- ohl, 2005) the coefﬁcients Ak deﬁne a stable VAR iff the ic polynomial is invertible on the unit disc |z| ≤ 1 in the ane. Deﬁning the spectral radius of the VAR as
x =0
{|z|−1} (3)
ent condition for stability—i.e. that (1) deﬁne a stationary process—is
(4)
covariance sequence �k for a covariance-stationary (not VAR) stochastic process ut is deﬁned as the sequence rices
, ut−k) k = . . ., −2, −1, 0, 1, 2, . . . (5) y stationarity the �k do not depend on the time t, and k � for all lags k. For a VAR process of the form (1), it may ppendix A.1, (A5)] that in the long run, the autocovari-
nce decays exponentially with lag k, the rate of decay the spectral radius �(A). Note, importantly, that this t if the autocovariance sequence for a process does not )exponentially then the process may not be modelled e Section 3.3). This has some importance for analysis of , which we return to in Section 4.2. R process (1), the autocovariance sequence is related
parameters via the Yule-Walker equations (Anderson,
�k−� + ık0� k = . . ., −2, −1, 0, 1, 2, . . . (6)
y, there are efﬁcient algorithms available for solving (6) terms of (Ak, �) [Appendix A.1, (A5)] and, conversely, n terms of the �k [Appendix A.1, (A6)]. s-power spectral density (CPSD) is deﬁned as the (two- ier transform of the autocovariance sequence3:
�ke −ik� 0 ≤ � ≤ 2� (7)
�
�
S(�)ei�k d� k = . . ., −2, −1, 0, 1, 2, . . . (8)
y, (7) and (8) may be efﬁciently computed by a (discrete) r Transform (FFT) and Inverse Fast Fourier Transform ctively.
icity, all spectral quantities are deﬁned on the normalised fre- 0 ≤ � ≤ 2�. In an empirical scenario—and indeed in the MVGC n—the natural frequency range of deﬁnition is 0 ≤ � ≤ f where f is the ency in Hertz (Hz). In fact, to take advantage of inherent symmetries, putes all spectral entities only up to the Nyqvist frequency f/2.
While terms that, g and m to (10) a matr
A fu the CPS (Gewe for alm
c−1I ≤
where itive se summ Rozano
Eqs varian mathe a VAR a data. A use of tationa freque
2.2. U
U t = (
( Xt
p∑ k=1
)−1 0 ≤ � ≤ 2� (10)
is no known closed-form solution of (9) for H(�), � in ), a classical result (Masani, 1966; Wilson, 1972) states a CPSD S(�) of a VAR process, a unique solution exists4
computed numerically [Appendix A.1, (A7)]. According AR coefﬁcients Ak may then be recovered from H(�) by ersion and inverse Fourier transform.
r technical condition for valid G-causality analysis is that uniformly bounded away from zero almost everywhere 982, 1984). Explicitly, there must exist a c > 0 such that ll 0 ≤ � ≤ 2�
≤) cI (11)
, B square matrices, ≤A B denotes that B − A is pos- eﬁnite. Importantly, condition (11) guarantees square y of the regression coefﬁcients (Geweke, 1982, 1984; 67). (10) relate the VAR parameters (Ak, �), the autoco-
quence �k and the CPSD S(�). Importantly, these three cal objects specify entirely equivalent representations for ny of them may be estimated from empirical time series
shall see (Section 3), the MVGC toolbox makes extensive equivalences to furnish accurate and efﬁcient compu- hways for the calculation of G-causality in the time and omains.
itional G-causality in the time domain
plest (unconditional, time-domain) form, G-causality d as follows: suppose that U t is split into two jointly
(i.e. inter-dependent) multivariate processes:
(12)
edictive interpretation (cf. Section 2), the G-causality , written FY→X , stands to quantify the “degree to which Y helps predict X , over and above the degree to which
predicted by its own past”. In the VAR formulation, this erationalised as follows: the VAR(p) (1) decomposes as
p
=1
) (14)
isely, given S(�) there is a unique holomorphic matrix function H˜(z) on lane with H˜(0) = I, and a unique symmetric positive deﬁnite matrix ) holds for H(�) ≡ H˜(e−i�).
L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68 53
The x-component of the regression (13) is
Xt = p∑
p∑ k=1
Axy,k · Y t−k + εx,t (15)
from which we see that the dependence of X on the past of Y , given its own past, is encapsulated in the coefﬁcients Axy,k; in par- ticular, there is no conditional dependence of X on the past of Y iff Axy,1 = Axy,2 = · · · = Axy,p = 0. This leads to consideration of the reduced (or restricted) regression—as opposed to the full regression (15)—given by omitting the “historic” (past) Y dependency:
Xt = p∑
k=1 A′xx,k · Xt−k + ε′x,t (16)
so that now the reduced sion residua stands to qu resents a “b (16). Maxim natural fram In this fram models of t for its more nically, it is
H0 : Axy,1 = in (15). This the appropr us that the of the form alised varian of the resid domain) G- ratio
FY→X ≡ ln
where �xx = ance matri predictive i of a regress tion error.6
the reductio included in
An impo causality is its interpre for signiﬁca essentially with other G
5 Alternativ or Lagrange m ratio is that is tions (Barrett decomposition tation as a tra et al., 2009; Ba
6 It may see as a measure o ance, frequenc for consistenc priate. See Bar
have a natural interpretation in terms of information-theoretic bits- per-unit-time. This is because, for a large class of joint processes, G-causality and information-theoretic transfer entropy (Schreiber, 2000) are asymptotically equivalent (Barnett and Bossomaier, 2013) [in the Gaussian case the equivalence is exact (Barnett et al., 2009)]. Note that transfer entropy is conceptually isomorphic to G- causality and is often described as a measure of information ﬂow (but see e.g. Lizier and Prokopenko, 2010). Thus, G-causalities may be meaningfully compared, and magnitudes cited, albeit with due caution to s
2.3. Condit
The unco undesirable
pend en sp
is n ) dep ty m y “c e av now in ge onfo een m t al., llust , rec epen
Xt
≡ ln
w the FY→ s pre ted b e tha ndit ; tha
that usly th iven i
X is predicted by its own past only. Here A′ xx,k
are regression coefﬁcients and ε′x,t the reduced regres- ls, with covariance matrix �′xx ≡ cov(�′x,t). FY→X , then, antify the degree to which the full regression (15) rep- etter” model of the data than the restricted regression um likelihood (ML) theory (Edwards, 1992) furnishes a ework for the analysis of parametric data modelling.
ework, an appropriate comparative measure for nested he form (15) and (16) is the likelihood ratio statistic or,
convenient statistical properties, its logarithm.5 Tech- a test statistic for the null hypothesis of zero causality
Axy,2 = · · · = Axy,p = 0 (17) motivates the deﬁnition of the G-causality statistic as iate log-likelihood ratio. Now standard ML theory tells likelihood for a realisation of length m of a VAR model
(1) is proportional to |�|−(m−p)/2, where |�|, the gener- ce of the model (Barrett et al., 2010), is the determinant uals covariance matrix. Thus the (unconditional, time- causality from Y to X is deﬁned to be the log-likelihood
|�′xx| |�xx| (18)
cov(εx,t) and �′xx = cov(�′x,t) are the residuals covari- ces of the VAR models (15) and (16) respectively. A nterpretation of (18) is that the generalised variance ion model may be viewed as a measure of model predic- From this perspective, G-causality (18) thus quantiﬁes n in prediction error when the past of the process Y is
the explanatory variables of a VAR model for X . rtant and frequently misunderstood point is that G-
often perceived to lack quantitative meaning beyond tation as a hypothesis-test statistic appropriate only nce testing (Section 2.5); that is, that its magnitude is meaningless, and in particular should not be compared -causality statistics. However, G-causality magnitudes
e test statistics which appear in the G-causality literature are the Wald ultiplier estimators (Edwards, 1992). An advantage of the likelihood has pleasant invariance properties under a broad class of transforma- et al., 2010; Barnett and Seth, 2011) (Section 2.7), a natural spectral
(Geweke, 1982) (Section 2.6) and an information-theoretic interpre- nsfer entropy (Schreiber, 2000; Kaiser and Schreiber, 2002; Barnett rnett and Bossomaier, 2013). m intuitive to use the mean square error trace(�) (the “total error”) f prediction error. However, on the grounds of transformation invari- y decomposition and information-theoretic interpretation, as well as y with the ML formulation, the generalised variance is more appro- rett et al. (2010) for a fuller discussion.
cal) de say, th if there lagged causali nated b they ar on unk it will tially c have b (Guo e
To i known inter-d
Xt = ∑ k
Xt = ∑ k
FY→X|Z
Not and co variate
tatistical signiﬁcance (Section 2.5).
ional and pairwise-conditional G-causality
nditional G-causality statistic introduced above has the characteristic that if there are joint (possibly histori- encies between X and Y and a third set of variables, Z urious causalities may be reported. Thus, for instance, o direct causal inﬂuence Y → X but there are (possibly endencies of X and Y on Z then a spurious Y → X ay be reported. These spurious causalities may be elimi- onditioning out” the common dependencies – provided ailable in the data. If, however, there are dependencies n (exogenous) or unrecorded (latent) variables, then neral be impossible to eliminate entirely their poten- unding effect on causal inference, although attempts ade to mitigate their impact [e.g. “partial” G-causality
2008)7]. rate the conditional case, suppose that the universe U of orded variables splits into three jointly distributed (i.e. dent) multivariate processes
(19)
h to eliminate any joint effect of Z on the inference of lity from Y to X . Again the VAR(p) (1) splits analogously
we may consider the full and reduced regressions
x,k · Xt−k + p∑
p∑ k=1
x,k · Xt−k + p∑
k=1 A′xz,k · Zt−k + ε′x,t (21)
o (15) and (16), but with the conditioning variables Zt both regressions. The null hypothesis to be tested is still e causality Y → X conditioned on Z, which we write gain as in (18):
|�′xx| |�xx| (22)
inclusion of Z in both regressions accounts for its joint X|Z may thus be read as “the degree to which the past of dict X , over and above the degree to which X is already y its own past and the past of Z”. t in this (and the previous) section, the source, target
ioning variables X , Y , Z may themselves be multi- t is, they may represent groups of variables. It is in this
there are a number of errors in this paper: ﬁrstly, it is stated at partial G-causality may be negative. Secondly, the sampling distri- s incorrect. See Roelstraete and Rosseel (2012) and also Barrett et al.
54 L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68
sense that we use the term “multivariate” G-causality. G-causality is thus able to account for group interactions. This is signiﬁcant, since elements in a multivariate system may function cooperatively or competitively, or interact generally in a more complex fashion than traditional bivariate analysis can accommodate (Ladroue et al., 2009; Barrett et al., 2010).
A case of particular importance is that of pairwise-conditional G-causality. Given a universe U of variables comprising n (known, recorded) jointly distributed univariate processes U1t, . . ., Unt, it is frequently of interest to estimate the G-causalities between pairs of var G-causalitie effects thro it is general causalities
Gi,j(U) ≡ FU
where the s in the multi ity Uj → Ui t are conditio as a weight the (G-)cau
2.4. Estima
So far w stochastic p time series now turn t tion, of FY→ The ﬁrst sta regression ( as the Akai (McQuarrie balance the mined by t ﬁt—perhaps time avoidi
The nex ters which VAR model (Section 1), reduced mo a choice of t to the ML es 1994) and v quently un Whittle, 19 see Append parameters sample esti covariance tors �ˆxx(u) the time do
An impo estimation is not clear
8 Confusing literature to th
9 This sectio 2.2); we need
VAR. In fact condition (11) guarantees that it is.10 However, even if the full VAR (1) is of ﬁnite order, the reduced VAR (16) will gen- erally be of inﬁnite order11 (similar remarks apply to the reduced regression (21)). Since under the ML formalism the full and reduced model orders should be the same, this implies that the appropriate (ﬁnite) empirical model order p should really be estimated for the reduced, rather than the full regression model. Failure to do so has potentially serious implications for G-causal inference (Section 2.5 – see also Section 3.1.2). The MVGC toolbox overcomes this issue in a natural way by only requiring estimation of the full regression
n 3.1).
atisti
nex ted c d, if ood r 1943 e co and mbe
redu esis tor s . Und pto
eter may laced he ca tive
stat xp(F̂ tribu tive on w ity. F ferab ). The F-tes ce G- ely b l asy ual c ques
very ot be cal s ce. F nder , 198 ence itable 3)]. ally, n vie t fam
oted onditi s that ced V ct if der VA
iables Ui, Uj, i /= j. The traditional bivariate pairwise s are the FUj→Ui . Since these are prone to spurious ugh joint dependencies as described above, however, ly preferable to consider rather the pairwise-conditional
j→Ui |U[ij] (23)
ubscript [ij] denotes omission of the ith and jth variables variate universe U . Thus, when considering the causal- he joint dependencies of all remaining known variables ned out.8 The quantities Gij(U), i /= j may be considered ed directed graph, which we shall sometimes refer to as sal graph.
tion from time series data
e have discussed time series data in terms of abstract rocesses; that is, we have assumed that our empirical data behaves like a realisation of some VAR process. We o the crucial issues of estimation, under this assump-
X|Z from9 a numerical time series u = u1, . . ., um. ge is to determine an appropriate model order for the 1). This may be achieved via standard techniques such ke or Bayesian information criteria, or cross-validation
and Tsai, 1998). The idea of model order selection is to number of parameters (in the VAR case this is deter- he maximum lag p) so as to achieve the best model
in a ML or error-minimisation sense—while at the same ng overﬁtting a ﬁnite data sequence. t stage is to obtain estimates of the model parame- maximise the likelihood function for the respective s (equivalently, minimise model error). As mentioned
the MVGC toolbox obviates the need to estimate the del parameters separately from the data. Here there is echniques yielding estimates asymptotically equivalent timate, notably ordinary least squares (OLS) (Hamilton, arious multivariate extensions of Durbin recursion (fre- der the banner of “LWR algorithms”) (Levinson, 1947; 63; Wiggins and Robinson, 1965; Morf et al., 1978) – ix A.1, (A2). Once we have estimates of all relevant
for both the full and reduced models, the G-causality mator F̂Y→X|Z(u) is obtained via (22) with the residuals matrices �xx, �′xx replaced by their respective estima- , �ˆ′xx(u). Another route to estimation of causalities in main is via spectral G-causality (Section 2.6). rtant but rarely considered issue in G-causality model is that, given that the full process ut is a VAR (1), it
that the subprocess Xt will always be a well-deﬁned
ly, the term “multivariate” G-causality is sometimes applied in the is case. n and the following applies equally to the unconditional case (Section simply take Z to be empty; i.e. of dimension 0.
(Sectio
2.5. St
The estima ity, an likelih Wald, both th Y ) = ny the nu nested hypoth estima bution an asym param (which be rep
In t alterna R2-like esis, e d2) dis alterna tributi causal be pre bution to the
Sin positiv centra the act techni
For may n empiri inferen test (A (Efron conﬁd nes su A.1, (A
Fin ered, i accoun
10 As n satisfy c it follow the redu 11 In fa
ﬁnite-or
cal inference
t task is to establish the statistical signiﬁcance of the ausality against the null hypothesis (17) of zero causal- desired, conﬁdence intervals for its magnitude. As a atio test, standard large-sample theory (Wilks, 1938; ) applies to the time-domain G-causality estimator in nditional and unconditional cases. If dim( X) = nx, dim( dim( Z) = nz (with nx + ny + nz = n) then the difference in r of parameters between the full model (20) and the ced model (21) is just d ≡ pnxny. Thus under the null
(17) of zero causality, (m − p) F̂Y→X|Z(u), the G-causality caled by sample size has an asymptotic  2(d) distri- er the alternative hypothesis the scaled estimator has tic noncentral- 2(d ;  ) distribution, with noncentrality = (m − p) FY→X|Z equal to the scaled actual causality , for the purpose of constructing conﬁdence intervals,
by its estimator). se of a univariate causal target [i.e. dim( X) = nx = 1] an asymptotic sampling distribution is available for the istic exp(FY→X|Z) − 1: namely, under the null hypoth- Y→X|Z(u)) − 1 scaled by d2/d1 has an asymptotic F(d1, tion where d1 = pny and d2 = m − p(n + 1), and under the hypothesis an asymptotic noncentral-F(d1, d2 ;  ) dis- here again   is equal to the actual scaled value of the or small samples in particular, the F-distribution may le (it has a fatter tail than the corresponding  2 distri-
MVGC toolbox makes both tests available and defaults t if the target variable is univariate. causality is non-negative, F̂Y→X|Z(u) will generally be iased, and under the alternative hypothesis the non-
mptotic distributions will be more accurate the closer ausality is to zero (Wald, 1943; Geweke, 1982). Some
for dealing with bias are discussed in Section 3.4. small samples the theoretical asymptotic distributions
sufﬁciently accurate and nonparametric data-derived ampling distributions may be preferable for statistical or this purpose the MVGC toolbox supplies permutation son and Robinson, 2001) and non-parametric bootstrap 2) routines for signiﬁcance testing and computation of intervals respectively; the toolbox also features routi-
for simulation of surrogate time series data [Appendix
if multiple causalities are to be simultaneously consid- w of the multiple null hypotheses we should take into ily-wise error rates or false discovery rates (Hochberg
by Geweke (Geweke, 1982), since the full CPSD S(�) is assumed to on (11), so too does the CPSD Sxx(�) of the process X t alone, from which
the coefﬁcients of the autoregression of X t are square summable, and AR is thus well-deﬁned. See Geweke (1982, 1984) for full details. U t is a ﬁnite-order VAR, then a subprocess X t will in general be a RMA process.
L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68 55
and Tamhane, 1987); again, this functionality is available in the MVGC toolbox.
2.6. G-causality in the frequency domain
A powerful feature of G-causality is that it may be decomposed in a natural way by frequency (Geweke, 1982, 1984). The resulting spectral G-causality integrates to the time-domain causality previ- ously introd all frequenc uncondition processes X
S(�) = (
Sx
Sy
Sxx(�) = Hx + H
f Y→X (�) ≡
or, in terms
f Y→X (�) ≡
where the p
1 2�
∫ 2� 0
f Y
so that tim over all freq
Sample approaches ters Ak, � f model orde Fourier tran ing to (10), from (29). A is then obta Note that n dure (Dham
12 In fact e �yx�
−1 xx Axy(�)|
In practice, ac satisﬁed.
from the data – many standard techniques are available to this end. Then, as mentioned previously (Section 2.1), H(�) and � may be computed numerically and an estimate for the spectral G-causality is again obtained from (28). The MVGC toolbox facilitates both techniques; we recommend the former due to improved numerical accuracy and computational efﬁciency (Section 3.1 and Appendix A.1). As regards statistical inference, in contrast to the time-domain case there are (to our knowledge) no known (asymptotic) distribu-
r the sample distribution of fˆ Y→X (�) (see Geweke, 1984 for discussion on this issue) and nonparametric techniques are ployed for signiﬁcance testing and derivation of conﬁdence ls13.
con nto s d reg
= ∑ k
(�) ≡ e spe
btai esti ion— ad to stima
be s ion o rma e eff ty m little t no ce. in t ties
) ≡ f
ears a “block g to o ral eq ible t
uced, which may thus be considered an average over ies of the spectral causality. The decomposition in the al case is derived as follows: a split of U into sub- , Y as in (12) induces a decomposition
x(�) Sxy(�)
x(�) Syy(�)
) (24)
s-power spectral density (Section 2.1) and a similar ion for the transfer function H(�). Now Sxx(�) is just
the sub-process X , and from (9) we may derive
x(�)�xxHxx(�) ∗ + 2Re{Hxx(�)�xyHxy(�)∗}
xy(�)�yyHxy(�) ∗ (25)
eweke (1982), we note that in the special case that ich may always be effected by a linear transformation
leaving FY→X invariant (Barrett et al., 2010), (25) takes form
(�)�xxHxx(�) ∗ + Hxy(�)�yyHxy(�)∗ (26)
e CPSD of X splits into an “intrinsic” term and a “causal” otivates deﬁnition of the (unconditional) spectral G-
m Y to X as
ln
( |Sxx(�)|
ln
( |Sxx(�)|
artial covariance matrix �y|x is deﬁned by
− �yx�−1xx �xy (29) n establishes the fundamental spectral decomposition ity in the unconditional case12
→X (�)d� = FY→X (30)
e domain causality may be thought of as the average uencies of spectral causality. estimation of f Y→X (�) admits at least two possible . A straightforward procedure is to estimate parame- or the VAR(p) (1) as before (after selecting a suitable r). The transfer function may then by calculated by (fast) sform of the estimated regression coefﬁcients accord-
the CPSD from (9) and the partial covariance matrix n estimate for the unconditional spectral G-causality ined by plugging the appropriate estimates into (28). o reduced regression is required. An alternative proce- ala et al., 2008a,b) is to estimate the CPSD S(�) directly
quality in (30) holds strictly when the condition |Ayy(�) − /= 0 is satisﬁed on 0 < � ≤ 2�; otherwise it should be replaced by ≤. cording to Geweke (1982), the equality condition is “almost always”
tions fo a fuller best de interva
The splits i reduce(
again o To
regress may le after e it may format transfo may b causali Again, so tha inferen
As causali
13 Our tion for f causality This app 14 The
accordin the integ It is poss
ditional case is less straightforward. Suppose that U ub-processes X , Y , Z as per (19). We then consider the ression [cf. (21)]
p
=1
( A′
e residuals X†, Z† as new variables. In Geweke (1984)
Y⊕Z†→X† (32)
Y t Z†t
) , which expresses the con-
e-domain G-causality FY→X|Z as an unconditional terms of the new variables X†, Y ⊕ Z†. Accordingly, l causality in the unconditional case is deﬁned as
f Y⊕Z†→X† (�) (33)
ctral decomposition
→X|Z(�) d� = FY→X|Z (34)
ns. mate fˆ Y→X|Z(�) in sample, a separate reduced which, as has been pointed out (Chen et al., 2006a),
substantial inaccuracies14—may be avoided as follows: tion of VAR parameters for (1) (or spectral estimation), hown [Appendix A.1, (A11) and (A12)] that the trans- f variables X , Z → X†, Z† deﬁned by (31) induces a
tion of the autocovariance sequence and CPSD which ected computationally. Then the conditional spectral ay be calculated as for the unconditional case via (33).
is known of the sampling distribution of f Y → X|Z(�) nparametric techniques must be used for statistical
he time-domain case, pairwise-conditional spectral may be calculated as
Uj→Ui |U[ij] (�) (35)
variance of G-causality
previously shown that multivariate G-causality, condi- nconditional and in both time and frequency domains,
lished simulation studies suggest that, in fact, the sampling distribu- ) at any ﬁxed frequency � is actually the same as for the time-domain , although we do not yet have a theoretical justiﬁcation for this claim. lso to hold in the conditional case.
decomposition” technique proposed in Chen et al. (2006a) appears, ur unpublished simulations, to yield inaccurate results. In particular, uality (34) generally fails even for simple test data in large samples.
hat the analytic derivation in Chen et al. (2006a) may be ﬂawed.
56 L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68
is theoretically invariant under the application of (almost) arbitrary stable, invertible15 digital ﬁlters (Antoniou, 1993; Barnett and Seth, 2011). Empirically, however, ﬁltering of stationary time series data may severely compromise statistical inference of causalities esti- mated in sample. This is due primarily to an increase in empirical VAR model order following ﬁltering. An important implication is that G-causality restricted to particular frequency bands cannot be measured by ﬁrst ﬁltering the data within the desired bands. While the computed values may change following ﬁltering, this likely reﬂects not ous values a inference. T avoid ﬁlteri spectral cau of interest m causalities and (33) ov (34)] to obt
FY→X (B) ≡
FY→X|Z(B)
where �(B nonparame band-limite
In pract where the d causal anal (Section 3.3 to improve tronically r ﬁnite differ ﬁlters of th (approxima constitutes not be diffe neuroscienc Magnetic R been previo
3. MVGC T
Central (Section 2. ance seque representat these equiv for moving thus furnish of G-causal frequency d mented in t
3.1. Compu
The basi the MVGC t
15 In Barnett a ﬁlter need on imum phase ﬁ for the ﬁltered pointing this o
order) a VAR (1) is ﬁtted to the time series data just once (A2) and all subsequent calculations are based on the estimated model parameters Aˆk, �ˆ. This approach is theoretically consistent with the fundamental assumption of G-causal estimation, that the time series data any of the initial estim spectral est (A2) proves
t; no tive ing e (2 ram Ther d re eters
(20 he d ticul t al., ainin eque
the ce se ll) V raigh tivel se t s rev eters hitt
ore a ilso
. The ant r gh, a d if d usal te w 15).
Redu varia noted l reg ill,
d reg ed a varia ode t, we n H( e VA nsfer r Tran s tha e spe ], wh (A7) he Ak oluti sion. s no e’s a oces
the desired “band-limited” G-causality but rather spuri- rising from inaccuracies in model ﬁtting and statistical he solution proposed in Barnett and Seth (2011) is to ng (of stationary data – see below) altogether. Then if salities are required, values outside the frequency range ay simply be ignored, while appropriate time-domain
may be obtained by averaging spectral causalities (28) er the frequency range B of prior interest [cf. (30) and ain band-limited G-causality:
1 �(B)
≡ 1 �(B)
) ≡ ∫ B d� is the measure (length) of B. Once again,
tric methods must be used for statistical inference of d G-causality. ice, some ﬁltering may still be appropriate in cases ata are not stationary to begin with, in which case G-
ysis is likely anyway to fail or deliver spurious results ). In these cases ﬁltering may be a valid and useful tool
stationarity; e.g. notch-ﬁltering of line-noise in elec- ecorded time series data (Barnett and Seth, 2011), or encing to eliminate drift (Seth, 2010). In the latter case, e form u˜t = ut − ut−1 may be applied iteratively until te) stationarity is achieved; but note that differencing
a non-invertible ﬁlter, so that stationary data should renced. An important application of ﬁlter invariance in e (Section 4) is to G-causal analysis of fMRI (functional
esonance Imaging) data, where serious confounds have usly suspected; this is discussed further in Section 4.3.
oolbox design
to the design of the MVGC toolbox is the equivalence 1) of the VAR parameters (Ak, �), the autocovari- nce �k and the cross-power spectral density S(�) as ions for a VAR process. The MVGC toolbox exploits alences to provide numerically accurate algorithms
ﬂexibly between the alternative VAR representations, ing computationally efﬁcient pathways for calculation ities, conditional and unconditional, in the time and omains. A schematic of computational pathways imple- he toolbox is given in Fig. 1.
tational strategy
c operating principle of estimation of G-causalities via oolbox is that (after determination of a suitable model
and Seth (2011) it is erroneously stated that to guarantee invariance ly be causal—i.e. not contain a pure lag. In fact full invertibility (min- ltering) is required to ensure that the condition (11) is not violated
process. We are grateful to Victor Solo (private communication) for ut.
efﬁcien alterna
Hav the tim VAR pa tively. reduce param in Seth from t be par Chen e for obt ance s or from varian the (fu and st respec is to u that, a param (A8) (W and m (A7) (W below) domin althou mente of G-ca integra (34) (A
3.1.1. autoco
As the ful order w reduce describ autoco ﬁnite m
Firs functio Now th the tra Fourie implie must b on [0, f rithm Then t tral res regres there i Whittl VAR pr
is a realisation of a stationary VAR (1). In principle, equivalent VAR representations might be chosen for ation—e.g. via sample autocovariance (A1 → A6) or
imation (A4 → A7)—but empirically direct estimation to be the most stable, accurate and computationally netheless the MVGC toolbox allows implementation of methods. acquired the VAR model, (conditional) G-causality in 2) and frequency (33) domains involves estimation of eters for the reduced regressions (21) and (31) respec- e is no simple algorithm (to our knowledge) to compute gression VAR parameters directly from full regression . Indeed, the standard approach [e.g. as implemented 10)] is to perform the reduced regressions explicitly ata (A2); however this leads to inaccuracies that may arly serious in the spectral case (see Section 2.6 and 2006a). Fortunately, numerical algorithms are available g reduced regression parameters from the autocovari- nce (A7) via solution of the Yule-Walker equations (6),
CPSD (A7) via spectral factorisation (9); the autoco- quence and CPSD may themselves be computed from AR parameters by “reverse” Yule-Walker solution (A5) tforward spectral calculation (Fourier transform) (A8) y. The recommended pathway for the MVGC Toolbox he autocovariance sequence. This is on the grounds ealed by empirical testing over a broad range of VAR , system size and model orders, Whittle’s algorithm le, 1963) for Yule-Walker solution is faster, more stable ccurate than Wilson’s spectral factorisation algorithm n, 1972); it also has additional useful properties (see
toolbox thus uses the autocovariance sequence as the epresentation for all G-causality routines (A13, A14) gain, the alternative spectral pathways may be imple- esired. Note that a useful “reality check” for the accuracy ity computations is that time-domain causalities should ith reasonable accuracy to their spectral counterparts
ced model order, spectral resolution and nce lags
previously (Section 2.4), even if the model order for ression (20) is ﬁnite (and in practice a ﬁnite full model of course, need to be selected), the model order for the ressions (21) and (31) will be in theory inﬁnite. If, as bove, reduced VAR parameters are calculated from the nce sequence (or CPSD) how do we choose a suitable l order?
note that spectral factorisation (9) speciﬁes the transfer �) at frequencies � for which the CPSD S(�) is speciﬁed. R coefﬁcients Ak are calculated by matrix inversion of
function followed by Fourier transform. Since the Fast sform (FFT) (Cooley and Tukey, 1965) will be used, this
t to calculate VAR coefﬁcients from the CPSD, the CPSD ciﬁed at some set of frequencies �1, . . . �q evenly spaced ere f is the sampling frequency—indeed, Wilson’s algo- assumes this, and approximates the H(�k) accordingly. will naturally be derived for k = 1, . . ., q; i.e. the spec-
on q will, effectively, be the model order for the reduced Unfortunately, for the spectral factorisation approach
obvious way to choose q. Furthermore (in contrast to lgorithm – see below), given a CPSD S(�) for a stable s evaluated, for a given spectral resolution q at evenly
L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68 57
Fig. 1. Schem outlined in Sec
spaced �k, Wilson’s alg afﬂict the a
For the a choice of q Whittle’s Y variance se calculates A VAR proces thus for lar This implie where the tolerance ( sion). Notin radius �—e parameters mentation tolerance, w racy).
Since th sequence (A sequent sp order for th ture of Whi
atic of computational pathways for the MVGC Toolbox. The “inner triangle” (shaded ci tion 2.1. Bold arrows represent recommended (useful and computationally efﬁcient) pat
there is no guarantee that VAR parameters derived by orithm will actually be stable. These problems do not
utocovariance approach, described next. utocovariance route, there is a natural answer for the , based on speciﬁc properties of VAR processes and ule-Walker solution algorithm (A6). Given an autoco- quence �k for k = 0, 1, . . . q lags, Whittle’s algorithm k for k = 1, . . . q. As noted in Section 2.1, for a stationary s the autocovariance sequence �k decays exponentially; ge enough k, �k will become numerically insigniﬁcant. s that we only need calculate the �k for k≤ some q, truncation decision is based on a speciﬁed numerical which might be determined, e.g. by machine preci- g that the exponential decay factor is just the spectral asily calculated from the (assumed known) full VAR —the MVGC Toolbox reverse Yule-Walker (A5) imple- core/var to autocov calculates q so that �q< a given hich defaults to 10−8 (empirically this yields good accu-
e CPSD is calculated by FFT from the autocovariance 9), q also becomes the spectral resolution for any sub-
ectral calculations. Thus a principled value for model e reduced regression is obtained. A further pleasant fea- ttle’s algorithm is that given the ﬁnite autocovariance
sub-sequen of lags q, th algorithm— model orde ria, and the decay, are e Toolbox, th ﬁtted VAR m
3.1.2. Comp The MV
designed w ever possib Matlab’s c appropriate multi-threa Fourier Tran scope of th
16 It is difﬁcu plexity and sca potential para
rcles) represents the equivalence between the VAR representations hways, while blue arrows represent actual MVGC calculation.
ce �k of a stable VAR model up to any given number e derived VAR parameters are—in contrast to Wilson’s guaranteed to be stable. We emphasise that the full r p as derived by standard model order selection crite-
reduced model order q as derived from autocovariance ffectively independent; as implemented by the MVGC e latter depends only on the spectral radius � of the odel.
utational efﬁciency and accuracy GC Matlab implementation (Appendix A) has been ith great regard to efﬁciency. All algorithms are wher- le completely vectorised and designed to exploit
omputational strengths, including the use, where , of key methods which invoke machine-optimised, ded libraries, such as linear solvers (LAPACK) and Fast sform (FFTW).16 Detailed benchmarking is beyond the
is article, but in testing we have not in general found
lt, in lieu of published information, to deduce the computational com- ling of Matlab’s core algorithms, particularly regarding the impact of llelisation.
58 L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68
performance to be a major issue, even with large, highly multivari- ate datasets.
The most performance-critical algorithm in the MVGC work- ﬂow under normal usage scenarios (Section 3.2) is likely to be the Yule-Walker solution algorithm (A6, core/autocov to var). The per- formance of this algorithm depends critically on the number of autocovariance lags deemed necessary for given numerical accu- racy – which, in turn, depends entirely on the spectral radius of the estimated VAR (see previous Section). Thus performance in practice is closely ti is that, if th “near unsta may well b dual-regres see Section autocovaria the acmaxl the acdect effectively note that st In another very long ti more efﬁcie may becom will thus be
Crucially case—multi (Section 2.6 tion is espe where resu inﬂuences b quency ban the data, os prior intere approach is G-causality tistical infer spectral cau 2006a), the duce spurio MVGC sing rectly (and above is ava
Regardin traditional theoreticall adverse eff comparativ traditional
Xt = aXt−1 Yt =
Here the re and unit-va (stability re G-causality analytically dently FX→ F(∞)Y→X , G-ca
17 This is mit covariance de the Ak tend to converges) for
method where inﬁnite lags are assumed for the reduced regression, and for F(1)Y→X , G-causality as approximated by the traditional dual- regression method, with model order 1 (i.e. 1 autoregression lag) for both full and reduced regressions.
We simulated the process (38) 10, 000 times with time series lengths of 100 time steps, for a = 0.8, b = 0.9 and varying causal coef- ﬁcient c, calculating sample distributions for G-causality estimators for both the statistically signiﬁcant causality Y → X and the statisti- cally non-s
2. W y und
of t sion
grea ing nt. t we reme
the
Xt + Yt +
x,t, unco oise ility he pr teps, s are ities tes t ll (no
neg using ance
VGC
’s “we C”, w
causa
ed to the actual dataset being analysed. An implication e spectral radius � is close to 1—i.e. the VAR estimate is ble”—then the single-regression approach of the MVGC e more computationaly intensive than the traditional sion method (“GCCA mode”, in the toolbox parlance –
3.3) and it may be necessary to limit the number of nce lags.17 This may be achieved either explicitly (via ags parameter) or by adjusting the decay tolerance (via ol parameter) in the routine A6, core/autocov to var, trading-off performance for numerical accuracy (but atistical inference may be compromised – see below). usage scenario—smaller spectral radius coupled with me-series lengths—the MVGC approach may actually be nt, since here VAR estimation (A2, core/tsdata to var) e a computational bottleneck, and a single regression
preferable. however, in arguably the most important usage variate conditional G-causality in the frequency domain )—performance is not the issue at stake. This calcula- cially relevant in neuroscience applications (Section 4), lts are required (a) to exclude common cross-variable y conditioning and (b) to be restricted to speciﬁc fre- ds. The traditional approach here has been to pre-ﬁlter tensibly to restrict causal estimates to frequencies of st; however, as already described (Section 2.7), this
fatally ﬂawed, insofar as it not only fails to restrict as desired, but in addition seriously compromises sta- ence. Thus it becomes essential to compute conditional salities where, as has been established (Chen et al.,
traditional dual-regression approach is known to pro- us results, including negative causal estimates. The
le-regression method handles this essential case cor- here too, the performance/accuracy trade-off described ilable). g accuracy, as previously mentioned (Section 2.4) the dual-regression method fails to take account of the y inﬁnite model order of the reduced regression, with ects on statistical inference. We illustrate this with a e analysis of the MVGC single-regression method vs. the dual-regression method for the minimal VAR(1)
+ cYt−1 + εx,t bYt−1 + εy,t
(38)
siduals εx,t, εy,t are normally distributed, uncorrelated riance white noise, a, b represent decay parameters quires that |a| < 1, |b| < 1) and c controls the strength of
from Y → X. G-causalities for (38) may be fully solved [Appendix B; see also Barnett and Seth (2011)]. Evi- Y ≡ 0 and in Appendix B we calculate expressions for usality as approximated by the MVGC single-regression
igated by the observation that model order estimates based on auto- cay for Whittle’s algorithm may be pessimistically large; in general,
become negligibly small (and the residuals covariance matrix � k� number of autocovariance lags.
in Fig. slightl
F̂(1)Y→X itself s be exp sion fa history disper
tributi by the sion in the me
cantly increas consta
Nex measu sidered
X˜t = Y˜t =
where terms ment n the ab lated t time s Result causal estima the nu II [false rates, signiﬁc regres is quit metho
3.2. M
18 Solo linear G Granger
igniﬁcant (“null”) causality X → Y. Results are displayed e see [Fig. 2(a)] that, due to ﬁnite-sample effects, F̂(∞)Y→X erestimates the true causality F(∞)Y→X on average, while tly underestimates the 1-lag causality F(1)Y→X – which ntially overestimates the true causality. The latter may d by the fact that the model order 1 reduced regres-
take into account the full explanatory power of the he process Xt on itself. We also see [Fig. 2(b)] that the (as measured by standard deviation) of the sample dis-
signiﬁcantly greater for F̂(1)Y→X ; this may be explained itional sampling error incurred by the extra regres- ed. In the non-causal X → Y direction, we see that both Fig. 2(c)] and standard deviation [Fig. 2(d)] are signiﬁ-
ter for F̂(1)X→Y than for F̂ (∞) X→Y ; while the latter decays with
causal strength c, the former remains approximately
investigated the robustness of G-causal inference to nt noise under the two estimation techniques: we con-
process
x,t
y,t (39)
y,t are normally distributed unit-variance white noise rrelated with the Xt, Yt, representing additive measure-
of intensity  . The addition of noise is known to degrade to detect G-causalities (Solo, 2007).18 Again we simu- ocess (39) 10, 000 times with time series lengths of 100
for a = 0.8, b = 0.9, c = 1 and varying noise intensity  . displayed in Fig. 3. Again we see that mean estimated as well as dispersion are higher for the dual-regression han the single-regression estimates, most severely in n-causal) X → Y direction. Here we also calculated Type ative, Fig. 3(c)] and Type I [false positive, Fig. 3(f)] error
the asymptotic null F-distribution (Section 2.5), at a level of  ˛ = 0.05. We see that while the impact of dual n Type II errors is negligible, the impact on Type I errors ere in comparison with the single-regression MVGC
work-ﬂow
al work-ﬂow for calculation of (conditional) G- from empirical time series data, as exempliﬁed in the
demo.m toolbox demonstration script, is illustrated as
rder estimation: For each model order p up to a chosen m, ﬁt a VAR model to the full “universe of data” time , . . ., um (A2) and calculate the likelihood L ∝ |�−(m−p)/2|.
ak linear GC” corresponds to our G-causality, as opposed to his “strong hich corresponds to what has been independently termed “partial lity” (see Section 2.3).
L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68 59
4 G-causality mean (causal)(a)
0
Fig. 2. MVGC (1) (38 statistics estim y F̂Y→ (inﬁnite lags) lag) ca signiﬁcant G-c ausali
Use this BIC). Sele
3. Autocov sequence of lags (a the Yule-
4. Time do [possibly (a) Calcu
regre �k (A (6).
(b) Calcu to (22
5. Frequen f Y → X|Z spectral (a) Trans
autoc calcu
Calcu cond Test by pe inter both
0
1
2
3
MVGC GCCA F (∞) F (1)
0
0.004
0.008
0
0.1
0.2
0.3
0.4
G-causality std. dev. (causal)(b)
0
0.006
0.012
0.018
(d)
(single-regression) vs. GCCA (dual-regression) accuracy for the minimal causal VAR ated over 10,000 runs of 100 time steps each. (a) sample mean signiﬁcant G-causalit G-causality as estimated by the MVGC method, while F(1) plots the theoretical (1- ausality; (c) sample mean null G-causality F̂X→Y ; (d) standard deviation of null G-c
to calculate the chosen model order criterion (AIC or ct the best model order p according to the criterion. el estimation: Estimate the corresponding VAR model rs (Ak, �) for the selected model order (A2) and check
spectral radius (3) is <1 (other statistical tests on parameters and residuals may be performed at this
(d)
(e)
• If
ariance calculation: Calculate the autocovariance �k from the VAR parameters (A5), to a suitable number s described above). This involves “reverse solution” of Walker equations (6). main: For each conditional causality FY→X|Z required
for the entire causal graph (23)]: late the VAR parameters for both the full and reduced ssions (20) and (21) from the autocovariance sequence 6). This involves solution of the Yule-Walker equations
late the time-domain conditional G-causality according ) (A13). he resulting causalities for signiﬁcance at a given level
the analytical sampling distribution (Section 2.5), tak- are to adjust for multiple hypotheses, and construct dence intervals if desired. cy domain: For each conditional spectral causality (�) required [possibly for all pairwise-conditional
causalities]: form the autocovariance sequence for X , Z to the ovariance sequence for X†t , Z
† t (A11); then, as per (33),
late the unconditional spectral causality f Y⊕Z†→X† (�). late the VAR parameters for the full regression (15) from ansformed autocovariance sequence (A6). late the transfer function H(�) by Fourier transforma- f the regression coefﬁcients (10), and then the CPSD ccording to (9) (A8).
we can integra
3.3. Potenti
Users of box (Seth, occasionall satisfactory nings when be because ments and be situation can be usef “GCCA mod traditional culation of Generally, h in GCCA mo with cautio
Likely re colinearity, moving ave discuss eac
(i) Colinea variable time se
1 2 3 4 causal coefficient
MVGC GCCA
G-causality std. dev (null)
MVGC GCCA
), with a = 0.8, b = 0.9 and varying causal coefﬁcient c (x-axis). Sample
X , as estimated by MVGC and GCCA methods. F(∞) plots the theoretical usality as estimated by the GCCA method; (b) standard deviation of ty. See text for details.
late the partial residuals covariance (29) and then the itional spectral causality according to (28) (A14). the spectral causalities for signiﬁcance at a given level rmutation test (Section 2.5), and construct conﬁdence vals, if desired, by non-parametric bootstrap.
time and frequency domain causalities are required,
alternatively calculate the time-domain causalities by ting their spectral counterparts (34) (A15).
al problems and some solutions
the GCCA (Granger Causal Connectivity Analysis) Tool- 2010), which the MVGC Toolbox supersedes, may
y ﬁnd that time series data which appeared to yield results using the GCCA software trigger errors or war-
analysed via the MVGC Toolbox. This will typically the MVGC Toolbox is more stringent in its require-
performs more thorough error-checking. There may s (see below) where the more robust GCCA approach ul. The MVGC toolbox may optionally be deployed in e” (see demo/mvgc demo GCCA), which takes the more approach of separate full and reduced regressions (cal- conditional spectral causalities will not be available). owever, data which yield apparently reasonable results de but fail in standard MVGC mode should be treated n. asons for reported problems with time series data are (i)
(ii) non-stationarity, (iii) long-term memory, (iv) strong rage component and (v) heteroscedasticity. We brieﬂy h in turn.
rity occurs when there are linear relationships between s (i.e. between individual time series) in multivariate ries data. In this case there is an ambiguity in the VAR
60 L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68
1.2
0.08 G-causality mean (null)(d)
Fig. 3. MVGC intensity   (x- methods. F(∞) method; (b) st null G-causalit ˛ = 0.05. See te
represe most lik reporte the core caller (s ear dep (PCA) (J 2013), o Compon
(ii) The prin box (exp as part core/var full VAR not sati tine cor and also reports core/var
Non-s Stationa niques s
MVGC GCCA F (∞) F (1)
0
0.02
0.04
0
0
0.1
0.2
G-causality std. dev. (causal)(b)
0
0.2
0.4
0.6
MVGC GCCA
0
0.2
0.4
0.6
0
(single-regression) vs. GCCA (dual-regression) accuracy for the minimal causal VAR(1) axis). Sample statistics estimated over 10,000 runs of 100 time steps each. (a) Sample m plots the theoretical (inﬁnite lags) G-causality as estimated by the MVGC method, while andard deviation of signiﬁcant G-causality; Type II error rate (proportion of false negativ y; (f) Type I error rate (proportion of false positives). Error rates are calculated using the xt for details.
ntation of the data. Colinearity (or near-colinearity) will ely be detected in the VAR estimation stage (A2) and
d as “rank-deﬁcient” or “ill-conditioned” regressions by /tsdata to var routine; this should be tested for by the ee demo/mvgc demo). One solution is to eliminate lin- endencies, possibly via a Principal Component Analysis olliffe, 2002) or factor model approach (Flamm et al., r a signal separation technique such as Independent ent Analysis (ICA) (Hyvärinen et al., 2001). cipal stationarity check performed by the MVGC Tool- licitly by the routine utils/var specrad or, more usually,
of the standard workﬂow [Section 3.2, step 3] by to autocov) is that the spectral radius of the estimated
model (1) is less than one (4). If this condition is sﬁed then analysis cannot proceed. Note that the rou- e/var to autocov performs exhaustive error checking
produces useful diagnostics: the utility utils/var info all errors, warnings and diagnostics generated by
to autocov (see e.g. demo/mvgc demo). tationary data may be dealt with in several ways. rity can sometimes be achieved by standard tech- uch as de-trending. As mentioned earlier (Section 2.7),
pre-ﬁlte differen underm root pro stationa G-causa Granger directly
An al ping or window implies better) a of wind varying particul trials: th using e assump indepen erative for an e
1 2 3 4 noise intensity
MVGC GCCA
G-causality std. dev (null)
MVGC GCCA
with additive noise (39), with a = 0.8, b = 0.9, c = 1 and varying noise ean signiﬁcant G-causality F̂Y→X , as estimated by MVGC and GCCA
F(1) plots the theoretical (1-lag) causality as estimated by the GCCA es); (d) sample mean null G-causality F̂X→Y ; (e) standard deviation of
asymptotic null F-distribution (Section 2.5) at a signiﬁcance level of
ring (e.g. notch ﬁltering of electrical line noise or ﬁnite cing) may also be useful, albeit at the potential cost of ining model ﬁtting and statistical inference. For unit cesses (processes exhibiting random walk-like non- rities) (Hamilton, 1994), a more principled route to l analysis is via co-integration (Granger, 1981; Engle and , 1987). The MVGC Toolbox is not currently able to deal
with co-integrated processes. ternative approach is to divide the data into (overlap-
non-overlapping) windows, on the logic that shorter s are more likely to be approximately stationary. This
a tradeoff between likelihood of stationarity (shorter is nd accuracy of model ﬁt (longer is better). An advantage owing—assuming there is sufﬁcient data—is that time-
G-causality can be analysed (Ding et al., 2000). This is arly useful given a large number of temporally aligned en so-called “vertical regression” can be implemented
xtremely short windows. This method depends on an tion (which requires justiﬁcation) that each trial is an dent realisation of the same underlying stochastic gen-
process; see the demo/mvgc demo nonstationary script xample.
L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68 61
(iii) The spectral radius condition may also fail if the time series, although stationary, has long-term memory—that is the auto- correlation does not decay exponentially. This may arise in particular for iEEG/LFP data (Section 4.2). In this case, the data is funda which m not curr memory sive Fra and Joy difﬁcult mine th checks o CPSD (c manifes
(iv) A probl contain compon core/var it may fa A.1, A5) GCCA m for fMR 2013) (s square s analysis causal m coefﬁcie ical sett order, so This ma compro
In rar Walker model o turn ou sufﬁcien converg and a re ance.
(v) Finally, assume depend is violat heteros ever, w be mod model w likely to ity can may co there is erature) Heteros which a and/or t models 4) the li scedasti
Future i above chall work (Barn equivalence predictive m within a ma
of analytic (asymptotic) sampling distributions for statistical infer- ence.
3.4. Debiasing of G-causality magnitudes
ausa is s cal s tely of bit y (Ba
accu strate o gen ws an nden ate d n be ufﬂe
lica
toolb tim
of w diffe ends is ma ler a , the e-ﬁlt
2.7) g at
evok , non umb
be i (com e ER on—s
indu s “th th th rial E ume n 3.3 oble of a ly de
mentally unsuited to VAR modelling (cf. Section 2.1), ay silently yield spurious results. The MVGC Toolbox is ently able to deal with such data. Models for long-term
processes do exist, e.g. VARFIMA (Vector Autoregres- ctionally Integrated Moving Average) models (Granger eux, 1980), but G-causal inference for such models is
and the theory underdeveloped. Pre-analysis to deter- e presence of long-term memory may be performed via n sample autocovariance (core/var to autocov) and/or ore/var to cpsd), where long-term memory typically ts itself as power-law behaviour. em may arise when the data, although stationary,
a strong (and in particular a “slow”) moving average ent, violating the condition (11). In this case, the routine
to autocov may report warnings or errors (speciﬁcally, il to solve the associated 1-lag problem – see Appendix
and it may become necessary to use the more robust ode described above. This may in particular be the case I BOLD data (Section 4.3). A recent study (Seth et al., ee also Section 4.3) indicates that empirically, even if ummability [or the condition (11)] is violated, G-causal
may still, under some circumstances, yield meaningful agnitudes and directionality. Clearly, however, if the nts of a VAR are not square summable, then in an empir- ing they cannot be well-approximated at ﬁnite model
that the estimated VAR will inevitably be misspeciﬁed. y result in diminished accuracy of causal estimates and mised statistical inference (Section 2.5). e cases it is possible that, as regards the reverse Yule- calculation (A5) (Whittle’s algorithm), the reduced rder q, as determined by autocovariance decay, may t to be too small for the VAR coefﬁcients Ak to decay tly, preventing the residuals covariance matrix � from ing. This is unlikely to occur given a valid VAR model asonably small choice of autocovariance decay toler-
under standard VAR model scenarios, it is usually d that the variance of the residual terms does not
on the actual values of the process. If this condition ed, the process is said to be heteroscedastic. Note that cedasticity does not in itself violate stationarity. How- hile a stationary heteroscedastic process might well ellable in theory as an (inﬁnite order) VAR, such a ill not be parsimonious and statistical inference is
suffer; indeed, it is well-known that heteroscedastic- invalidate standard statistical signiﬁcance tests, and nfound G-causal inference (Luo et al., 2011). While
extensive research (mainly in the econometrics lit- into GARCH (Generalised AutoRegressive Conditional cedastic) models (Silvennoinen and Teräsvirta, 2009), utoregress residuals variances on their own history he history of the process itself, G-causal analysis of such is somewhat fragmented. In the neurosciences (Section terature on detection and functional analysis of hetero- city (Ozaki, 2012) is limited.
terations of the MVGC will address some or all of the enges. These iterations will take advantage of our recent ett and Bossomaier, 2013) establishing a very general
of G-causality and transfer entropy for a broad class of odels (e.g. VARMA, VARFIMA, GARCH, co-integration) ximum likelihood framework, enabling the derivation
G-c mation statisti accura terms entrop ensure As illu ing is t windo indepe surrog can the non-sh
4. App
Alth sis of t applica opmen review and Hu et al., involve MVGC science
4.1. Ap
M/E the hig nature ation i mentio variety can be term tr analys a simp Second mal pr Section ﬁlterin
For (ERPs) large n 3.3 can native averag variati ing the (i) risk ing wi inter-t
Vol (Sectio of a pr cation spatial
lity is by deﬁnition >0 and hence any empirical esti- ubject to bias. This is not important when assessing igniﬁcance but may be important if the objective is to quantify the magnitude of a G-causality interaction in s (leveraging the asymptotic equivalence with transfer rnett et al., 2009; Barnett and Bossomaier, 2013)). To rate magnitude estimation, debiasing is recommended. d in Barrett et al. (2012), a useful approach to debias- erate surrogate distributions by dividing the data into d (many times) randomly rearranging these windows tly for each variable. The mean G-causality across this ata set should give a good estimate of the bias, which
subtracted from the sample estimate obtained from the d data.
tion to neuroscience time series data
h G-causality is an entirely general method for analy- eries data, the MVGC toolbox has been developed with
to neuroscience data in mind. Methodological devel- this application domain is rapidly advancing and a full eyond the present scope (see, for example, Deshpande 12; Friston et al., 2013; Bressler and Seth, 2011; Ding ). This section summarises some of the main issues
application of G-causality (as implemented by the ox) to some of the more common varieties of neuro-
e-series data.
tion to surface EEG and MEG data
ata is well suited for analysis by G-causality in virtue of e resolution, fast sampling, and its typically stochastic
analysis of steady-state M/EEG the primary consider- nsure that the time-series are covariance stationary. As (Section 3.3), non-stationary data can be treated in a ays which may achieve stationarity. First, time-series renced (Section 2.7) which is useful for removing long-
or drifts; however the interpretation of any subsequent y be altered (linear or piecewise de-trending provide nd less problematic alternative that may be effective).
data can be windowed (Section 3.3). Finally, mini- ering may be applied to help achieve stationarity (see , for example by removing electrical line noise by notch 50 (or 60) Hz (Barnett and Seth, 2011). ed data, as exempliﬁed by event-related potentials stationarity is likely to be a common issue. Given a er of trials, “vertical regression” as outlined in Section mplemented using extremely short windows. An alter- plementary) approach is to subtract the ensemble
P from each trial, which—assuming low inter-trial ERP hould result in stationary residual time series reﬂect- ced response (Chen et al., 2006). However, this method rowing out the baby with the bathwater” by dispens- e ERP itself, and (ii) will fail if there is any substantial RP variability, which is common (Wang et al., 2008). conduction in EEG may lead to excessive colinearity ) among EEG time series in sensor space (this is less m for MEG). Some useful approaches here are appli-
surface Laplacian transform, which has the effect of correlating the data (e.g. Cohen and van Gaal, 2013),
62 L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68
use of factor models (Flamm et al., 2013) or projection of sensor- level VAR model coefﬁcients onto the locations of neural sources (Michalareas et al., 2013). The validity of application to source- localised EEG data depends on the method of source-localisation and is beyond the present scope (see Barrett et al., 2012 for an illustrative example). Finally, as emphasised earlier, spectral G- causality of M/EEG data should be measured by the band-limited approach (Section 2.7) and not by preﬁltering into the desired band- pass and then applying time-domain G-causality.
4.2. Applica
Intracran data also pr surface M/E sharing the rate. Howev sometimes variance on non-station neural and and which which are d well as the possibility f tage/referen and drifts, e fully adequ cially long- adapted me these cavea likely to be illuminatin
4.3. Applica
Applicat controversi Valdes-Sosa the hemody and variab mation of u Second, typ sample inte 1 to 3 s, su Inter-region devastating at the neu than the B analysis wo Perhaps su fMRI BOLD properties, This is bec ﬁlter, to w this turns as a delay proof toget lish this re
19 The proof t and the conseq evidence that fully invertible 2004) requires
include detailed population-based spiking neuron generative models coupled to the biomechanically realistic Balloon–Windkessel model of hemodynamic responses, elim- inating concerns that the invariance properties described above depend on responses.
Unfortu together w ence of BOL are combin
eed I da
g ultr ussen sence mend le by ity be tify s in tion ) co oeck riati
stat tivit econ ses ( ity o est b
al. (2 n.
nt pr s the
its o halle r mo ch is duce likel Kim ot su ly le roce en pr g the gad
each , cho mea sky e tion ed in etica
clus
tion to intracranial EEG and LFP data
ially recorded EEG (iEEG) and local ﬁeld potential (LFP) ovide data well suited for G-causality analysis. Unlike EG this data is usually spatially highly precise while
advantages of high temporal resolution and sampling er, perhaps counter-intuitively, LPF and iEEG data can appear to be “too clean” inasmuch as the stochastic
which VAR modelling depends is overshadowed by ary deﬂections and ﬂuctuations which may have both non-neural (e.g. due to electrode movement) origins, may reﬂect long-term memory effects (Section 2.1) ifﬁcult to accommodate within a VAR framework. As
techniques described above (for M/EEG), one additional or iEEG/LFP is to transform the data using a bipolar mon- ce which may reduce the impact of common sources mphasising the residual stochastic activity. However, a ate treatment of inherently nonstationary (and espe- term memory) data is likely to require substantially thods, e.g. VARFIMA modelling (Section 3.3). Despite ts, G-causality analysis of stationary iEEG/LFP data is
highly informative; see Gaillard et al. (2009) for an g example.
tion to fMRI BOLD data
ion of G-causality to fMRI BOLD data has been highly al for apparently good reasons; e.g. David et al. (2008),
et al. (2011). First, the BOLD signal (as captured by namic response function, HRF) is an indirect, sluggish, le (inter-regionally and inter-subjectively), transfor- nderlying neural activity (Handwerker et al., 2012). ical fMRI protocols involve severe downsampling with rvals (repetition times, TRs) normally ranging from bstantially longer than typical inter-neuron delays. al HRF variation has been argued to be particularly
for G-causality analysis: if X is causally driving Y ral level, but if the BOLD response to X peaks later OLD response to Y, the suspicion is that G-causality uld falsely infer that Y is driving X (David et al., 2008). rprisingly, this is not the case. In fact, G-causality of
data is robust to a wide variety of changes in HRF including notably their time-to-peak (Seth et al., 2013). ause the HRF is effectively a slow, moving average hich G-causality is in principle invariant. (Intuitively, on recognising that a convolution is not the same . We have recently provided a detailed theoretical her with a range of simulations which fully estab- sult19 (Seth et al., 2013). Notably, these simulations
urns on the identiﬁcation of the HRF convolution as an invertible ﬁlter uent ﬁlter-invariance of G-causality (Section 2.7). While there is good the HRF convolution is causal (Handwerker et al., 2004), whether it is
(i.e. minimum-phase) for typical HRF parameters (Handwerker et al., further research.
are ind to fMR portin (Rasm the ab recom possib causal to iden change as reac and (iv (Roebr HRF va tion of connec blind d proces causal may b Seth et domai
4.4. Ap
Poi resent carries main c able fo approa and re imum 2005; does n arguab point p may th treatin (Nedun volve kernel of the Kisper isfy sta is need a theor
5. Con
The routin quency optimi tical in
simpliﬁed VAR-based generative models of neuronal
nately, the severe downsampling imposed by fMRI, ith measurement noise, still undermine G-causal infer- D data in many applications. When confounding HRFs ed with these other factors, false G-causality inferences likely. This precludes naive application of G-causality ta generally. While technological developments sup- a-low TRs (Feinberg and Yacoub, 2012) and de-noising
et al., 2012) promise to alleviate these problems, in of these developments a conservative methodology is ed. Useful strategies include (i) using as short a TR as
compromising on coverage; (ii) examining changes in G- tween experimental conditions, rather than attempting “ground truth” G-causality patterns; (iii) correlating G-causality magnitude with behavioural variables such
times across trials (or trial blocks) (Wen et al., 2012), mputing the so-called “difference of inﬂuence” term
et al., 2005) which may provide some robustness to on. Alternative promising approaches include estima- e-space models which jointly parameterize functional y and hemodynamic responses (Ryali et al., 2011) or volution of the HRF to retrieve the underlying neuronal Havlicek et al., 2011; Wu et al., 2013). In general, G- f fMRI BOLD data should be treated with caution and e interpreted as exploratory (Friston et al., 2013). See 013) for further discussion of this important application
tion to spiking (i.e. point process) data
ocess data obtained from direct neural recordings rep- other end of the spatiotemporal scale from fMRI, but wn challenges with respect to G-causality analysis. The nge is that point process spike train data is not suit- delling by linear VAR models. A theoretically principled
to replace the VAR modelling step with ﬁtting of full d point process models, within a framework of max- ihood estimation (Okatan et al., 2005; Truccolo et al., et al., 2011; Gerhard et al., 2013). The MVGC toolbox pport this in its current version. A simpler (although ss principled) approach is to obtain an estimate of the ss spectrum of spiking neural data; G-causal analysis oceed via spectral factorisation (Section 2.1), effectively
estimated spectrum as if it derived from a VAR process i et al., 2009). A “quick and dirty” alternative is to con- spike train with a Gaussian or half-Gaussian (causal) osing the kernel width by some appropriate function n inter-spike interval – see e.g. Cadotte et al. (2008), t al. (2011). Assuming the resulting time series data sat- arity tests, G-causality can then be applied. Due caution
any interpretation of the results since this, again, is not lly principled solution.
ions
GC toolbox provides a comprehensive set of Matlab implementing G-causality analysis in the time and fre- ains and in both conditional and unconditional cases. It
omputational efﬁciency, numerical accuracy and statis- nce by leveraging multiple equivalent representations
L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68 63
of a VAR model by regression parameters, the autocovariance sequence, and the cross-power spectral density of the underlying process. By this approach, it is able to compute G-causality quan- tities by a “one shot” regression, without requiring separate “reduced” r estimation causality is functionalit mation, and environmen sive docum
As with via the MVG a good und practical co being the ca major chall of the funct its fruitful a systems, bo
Acknowled
We are g ments, and ﬁnancial su to the Dr. M supports th
Software from www for non-pro License (GP www.gnu.o
Appendix A
We now MVGC toolb ate time ser m and assum
u ≡ 1 m
m∑ t=1
A1 Sample
The sam
�ˆk(u) = m −
The factor o unbiased es pathway si biased by e routine imp to Shkolnis nonetheless is that it is d
will be appropriate to avoid overﬁtting while adequately modelling variation in the data.
Key routines: core/tsdata to autocov, experimen- tal/tsdata to autocov debias
R parameter estimation
cov
1 − p
seco hm”— ions e, 19 t al., es Aˆk dely t onl nce
are likeli cQua need ingle n equ
seri for d
step rou
egressions which reduce statistical power and model accuracy; the awkward case of conditional spectral G- also ﬂuently catered for. The toolbox provides extensive y for statistical signiﬁcance testing, model order esti-
error checking. It seamlessly integrates into the Matlab t, offering multiple demonstration scripts and exten- entation throughout. any advanced statistical method, G-causality analysis C toolbox should be implemented with care and with
erstanding of the underlying statistical principles and nstraints, as described in this paper and elsewhere. This se, we hope that the MVGC toolbox will help address a enge for current neuroscience, namely the deciphering ional organisation of neural systems. We also anticipate pplication in other contexts involving complex dynamic th within biology and beyond.
gements
rateful to Adam Barrett for helpful discussions and com- for testing pre-release versions of the MVGC toolbox. For pport we are grateful to the EPSRC (G/700543/1) and ortimer and Dame Theresa Sackler Foundation, which e work of the Sackler Centre for Consciousness Science.
availability: The MVGC toolbox may be downloaded .sussex.ac.uk/sackler. The software is freely available ﬁt academic usage under the GNU General Public L), version 3 or optionally any later version; see rg/licenses.
. MVGC algorithms
gorithms
review the algorithms A1–A15 implementing the ox key computational pathways (Fig. 1). All multivari- ies u1, . . ., um in what follows are n-variable of length ed already de-meaned; i.e. the sample mean
t (A.1)
ubtracted from each sample ut (see stats/demean). ll MVGC routines that reference time series data gen- t/return multi-trial data, here for clarity we assume
data. A model order p is assumed to have been selected.
autocovariance estimation
1 k − 1
ut · ut−k� k = 0, 1, 2, . . . (A.2)
f 1/(m − k − 1) rather than 1/(m − k) is used to obtain an timate. We do not recommend this as a computational nce (auto)covariance estimates can be unacceptably rror in estimation of the mean (A.1). An experimental lementing a potentially more accurate algorithm due ky et al. (Shkolnisky et al., 2008) is included, but we
do not recommend this pathway. Another drawback ifﬁcult to ascertain how many lags k of autocovariance
A2 VA
The square series Aˆ1, . . .
�ˆ = m
The algorit extens Whittl (Morf e matric It is wi that no covaria mator) use in BIC (M which for a s than a of time (1978)
Key
A3 VA
The multiv mally d Ap, �). ients, a autom mate s of time
Key
C toolbox supplies two algorithms for ﬁtting parame- for a VAR model (1). Both yield VAR parameter estimates mptotically equivalent to the corresponding ML esti-
method is a standard OLS, which computes a least- imate for the sample estimators Aˆk. Given a sample time . . ., um and estimated regression coefﬁcient matrices he residual errors for the regression are
p
1
Aˆkut−k t = p + 1, . . ., m (A.3)
, the Aˆk are chosen so as to minimise the mean or E2 = (1/(m − p))
∑m t=p+1‖εˆt‖
2 where ‖· ‖ denotes the n) vector norm. In the MVGC toolbox the OLS is imple- the Matlab “/” (mrdivide) operator, which solves the ined linear system
∑p k=1Aˆkut−k = ut , t = p + 1, . . ., m
-squares sense via QR decomposition. An estimate for ls covariance matrix is then obtained as the unbiased ariance of the residual errors [cf. Eq. (A.2)]:
− 1
m∑ t=p+1
εˆt · εˆ�t (A.4)
nd supported method is a variant of the so called “LWR a term generally referring to a class of multivariate to Durbin recursion (Levinson, 1947; Durbin, 1960; 63; Wiggins and Robinson, 1965)—due to Morf et al.
1978). The Morf variant estimates regression coefﬁcient recursively for k = 1, 2, . . . from the time series data ut. acknowledged to be very stable, and has the advantage y VAR coefﬁcients but also estimates �ˆ for the residuals matrix (and hence the VAR maximum likelihood esti- computed recursively. It is thus extremely efﬁcient for hood-based model selection criteria such as the AIC or rrie and Tsai, 1998), in comparison with OLS estimation, s to be recomputed for each model order. In practice,
estimate, the Morf algorithm may be faster or slower ivalent OLS, depending on number of variables, length es and model order. We refer the reader to Morf et al. etails of the algorithm. tines: core/tsdata to var, core/tsdata to infocrit
ulation
tine core/var to tsdata returns simulated multi-trial, e VAR(p) test data u1, . . ., um according to (1) with nor- buted iid residuals εt, for given VAR parameters (A1, . . ., al samples will generally contain non-stationary trans- ay thus be truncated. Truncation may be performed
lly; a sufﬁcient number of initial samples to approxi- narity is calculated according to the estimated number s for autocovariance to decay to near zero (A5). tine: core/var to tsdata
64 L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68
A4 Spectral estimation
Although we do not recommend spectral estimation as a starting point for MVGC estimation (Sections 3.1 and 3.1.1 – although see Dhamala et a useful pre
The rou methods of series data algorithm u 1998) whic modiﬁed pe the resultin sity estimat Toolbox fun parameters uses a mul from the Ch 2008). In ad bandwidth the user. D multi-taper the Matlab
Key rou
A5 Yule-W
Given V Yule-Walke Firstly, we n since (6) ex culated recu then we can
Conside Walker equ that �0 is a
�0 = A�0A�
This is a di 1972) for �0 Matlab Con the form (A the MVGC t iterative sol a standard on a new se
Upt = ApUpt− where
aut n 2.1 ) (A.6
(wh will d w ially
�(A rou
. .
. .
the
. . in sa ) coe eve riab puta e VA are n odel
al., 2008a) examination of cross-power spectra may be liminary step in time series data analysis. tine core/tsdata to cpsd implements two different
estimating the cross-power spectral density from time (further algorithms may be added in future). The ﬁrst ses Welch’s method (Welch, 1967; Oppenheim et al., h splits the data into overlapping “windows”, computes riodograms of the overlapping segments, and averages g periodograms to produce the power spectral den- es. It is implemented via the Matlab Signal Processing ctions pwelch and cpsd. Window length and overlap
may be speciﬁed by the user. The second algorithm ti-taper method (Percival and Walden, 1993) adapted ronux neural data analysis package (Mitra and Bokil, dition to window length and overlap parameters, a time parameter and the number of tapers may also be set by iscrete prolate spheroidal (Slepian) sequences for the
method are calculated using the function dpss from Signal Processing Toolbox. tine: core/tsdata to cpsd
alker reverse solution
AR(p) parameters (A1, . . ., Ap, �), we wish to solve the r equations (6) for the autocovariance sequence �k. ote that if the �k are known for k = 0, 1, . . ., p − 1, then
presses �k in terms of �k−1, . . ., �k−p, the �k may be cal- rsively for k ≥ p. Thus if we can solve (6) up to k = p − 1
calculate �k up to arbitrary lags. r now the case p = 1. Setting A ≡ A1, the ﬁrst two Yule- ations are �0 = A�1� + � and �1 = A�0, leading to (note
symmetric matrix)
+ � (A.5) screte-time Lyapunov equation (Bartels and Stewart, , for which efﬁcient numerical solvers are available. The trol System Toolbox function dlyap solves equations of .5); if the Control System Toolbox is not available, then oolbox function utils/dlyap aitr implements an efﬁcient ver. Thus we may calculate �0 for a VAR(1). We now use trick (Lütkepohl, 2005) to express a VAR(p) as a VAR(1) t of variables. Given the VAR(p) (1), we obtain the VAR(1)
1 + ε p t (A.6)
1
The (Sectio VAR(1
�p k
= ( Now it inal VA so tha matrix the au lags su erance the �k require potent regime
Key
and ( A1 .
mated VAR(q
How for n va be com the tru (A.13) true m
iduals covariance matrix is
�1 . . . �p−1
�0 . . . �p−2
� . . . �0
⎞⎟⎟⎟⎟⎠ (A.11) solving the discrete-time Lyapunov equation Ap� + ˙p for �p0 yields �1, . . ., �p−1 and the �k culated recursively up to arbitrary lags as described
ocovariance for a VAR decays exponentially with lag ); more precisely, the Yule-Walker equations for the ) yield
p 0 k = 1, 2, . . . (A.12)
asy to show that the spectral radius �(A) of the orig- is just the largest modulus of the eigenvalues of Ap, ‖ decays with rate �(A) where ‖· ‖ is any consistent
m. In the MVGC Toolbox routine core/var to autocov, ariance sequence is calculated up to a maximum of � at �(A)� is smaller than a given speciﬁed numerical tol- ich defaults to 10−8) beyond which it is presumed that be negligibly small. Thus the maximum number of lags ill depend on the spectral radius of the VAR, and may become very large as the VAR approaches the unstable ) → 1. tine: core/var to autocov
alker solution
n autocovariance sequence �k the Yule-Walker equa- ay be solved for VAR parameters (A1, . . ., Aq, �) under tion that �k is the autocovariance sequence of a VAR(q). ns for k = 1, . . ., q may be written as
( �1 . . . �q
· ( �1 . . . �q
)� . In fact, if the �k are esti-
mple (cf. A1), then (A.13) is just the OLS solution for fﬁcients (cf. A2). r, the solution (A.13) has the practical disadvantage that les it requires inversion of an nq × nq matrix, which may tionally prohibitive if q is large (cf. A5). Furthermore, if R model is stable and of order p, the Ak computed from ot guaranteed to be stable, even if q = p. In practice, the
order will not in general be known exactly; indeed, in
L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68 65
the MVGC approach, the �k will generally be calculated as in (A5) up to q large enough to ensure negligible autocovariance for k > q.
The MVGC Toolbox function core/autocov to var uses instead an LWR algorithm due to Whittle (Whittle, 1963) to solve (6) recur- sively for V algorithm, true VAR is �) is also s n variables sions, and i higher mod details.
Key rou
A7 VAR sp
Although known clos known that exists. The r due to Wil implementa jan (see also then be rec Fourier tran calculation.
We rem tional path a wide ran efﬁciently a domain (i.e tle’s LWR al
Key rou
Key rou
Calculat the Fourier terms of no FFT of a seq
F[G](�) = ∑ k
From (7) we
F−1[P]k = 2
we want to represent (8) as an integral over [0, 2�], rather than over [− �, �]. We may calculate that �k = (−1)kF−1[P]k k = 0, 1, 2, . . . (A.17)
S(� −
2.1) rou
olera
hav
q
ering te th
heim( � x
pri d reg s. Alt
AR parameters (A1, . . ., Aq, �). Importantly, Whittle’s unlike the OLS solution (A.13), guarantees that if the stable of order p, then the calculated model (A1, . . ., Aq, table, even if not of the correct order; i.e. if q /= p. For the algorithm requires 2q separate n × n matrix inver- s thus likely to be computationally tractable up to far el orders. The reader is referred to Whittle (1963) for
tine: core/autocov to var
ectral factorisation
as mentioned previously (Section 2.1) there is no ed-form solution of (9) for H(�), � in terms of S(�), it is
a unique solution to the spectral factorisation problem outine core/cpsd to var deploys an iterative algorithm son (Wilson, 1972) to achieve this numerically. The tion is based on code kindly provided by G. Rangara-
Dhamala et al., 2008a,b). The VAR coefﬁcients Ak may overed from H(�) by a matrix inversion and inverse sform; the utility function utils/trfun2var performs this
ark that this is not a recommended MVGC computa- way, since our tests indicate that, numerically, over ge of scenarios, spectral factorisation may be more nd accurately calculated by transforming to the time . to the autocovariance sequence (A10), and using Whit- gorithm (A6). tine: core/cpsd to var
ectral calculation
tion core/var to cpsd implements the relations (9) and to compute a VAR CPSD S(�) from VAR coefﬁcients (Ak,
ourier transform (FFT) is used to calculate the transfer �) from the VAR coefﬁcients. tine: core/var to cpsd
ansforming between the autocovariance sequence power spectral density
∞
thus ﬁnd:
(�) + F[�](�)∗ − �0 0 ≤ � ≤ 2� (A.15)
g in the other direction entails the inverse Fourier 8), implemented in the toolbox as an inverse fast Fourier IFFT). Since the (discrete) IFFT of a function P(�) approx-
1 �
with
P(�) =
Key
some t
and we
Xt = ∑ k
transfo
have S y
S(� − �) 0 ≤ � ≤ � S(� − �) � < � ≤ 2� (A.18)
S(�) on [0, �], which is what we require since the MVGC resents all spectral quantities up to the Nyqvist fre- ich corresponds to [0, �] for normalised frequencies (cf. . tines: core/autocov to cpsd, core/cpsd to autocov
utocovariance and spectral transforms for reduced
e the autocovariance sequence and S(�) the CPSD for a t of variables. We assume that we may take �k = 0 for
take q large enough that the autocovariance decays to
nce, as in A5). Suppose that U t splits as U t = (
Xt Y t
Xt−k + X†t (A.19)
noise) residuals X†t . Note that we we may restrict the ression to q lags, since from the Yule-Walker equations e seen that �k = 0 for k > q implies that Bk = 0 for k > q.
the residuals X†t as new variables, the problem is to e autocovariance sequence �†
k and CPSD S†(�) for the
d universe U†t = (
X†t Y t
) . Now since the X†t are residuals
sion they are white noise (i.e. iid and serially uncor- that �†
xx,k = ık0�†
xx , where �†
xx = cov (X†t ). We also have
and from (A.19) it follows that the �†xy and � † yx are
convolving corresponding � terms with B:
�† xx
xy,k to k = 2q lags
n practice it seems that q lags is sufﬁcient) and �† yx,k
the spectral domain, again since X† is white noise, f X† is just the ﬂat spectrum S†
xx (�) = �†
xx , and we also
= Syy(�). From (A.20), and the Convolution Theorem et al., 1998), we have:
† x
B(�)Sxy(�)
k=1Bke −ik� is the Fourier transform of the
ression coefﬁcients. ncipal application of the transformations is to the ression (31) in the calculation of conditional spectral hough the recommended pathway for this calculation
66 L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68
is via the autocovariance sequence, in fact the spectral transforma- tion (A.21) is more efﬁcient in practice (especially if q is large), since fast Fourier transformation may be deployed; in fact the default method in the routine core/autocov xform is to convert to the CPSD (A11), appl sequence (A for complet
Key rou
A.2. G-caus
A13 Time-
Time do sequence (A5) from gc/autocov and (22) in (A6) is used the full and calculates s
Key rou
A14 Frequ
Spectral variance core/var to the routine pairwise-co parameters core/autoco H(�) are the
In the c from the au (31) by cor autocovaria tional spect autocovaria according to [cf. A12, Eq. target varia the covaria
Key rou
A15 Integr
Key rou
Appendix B
We cons
A = (
H(z) = 1 (1 − az)(1 − bz)
( 1 − bz cz 0 1 − az
) (B.3)
| |1 −
c
− ab
y (A.21) and then convert back to the autocovariance 12). The function core/cpsd xform is also implemented eness. tines: core/autocov xform, core/cpsd xform
ality algorithms
domain G-causality calculation
main causalities are calculated from the autocovariance �k—most likely obtained via core/var to autocov
estimated VAR parameters—by the routine to mvgc according to (18) in the unconditional
the conditional case. The routine core/autocov to var to calculate the residuals covariance matrices for both
reduced regressions. The routine gc/autocov to pwcgc imilarly the pairwise-conditional causalities (23). tines: gc/autocov to mvgc, gc/autocov to pwcgc
ency-domain (unconditional) G-causality calculation
causalities are calculated from the autoco- sequence �k—again most likely obtained via
autocov (A5) from estimated VAR parameters—by s gc/autocov to smvgc or gc/autocov to spwcgc for the nditional case. In the unconditional case (28), VAR
(Ak, �) for the full regression are ﬁrst calculated by v to var (A6), and the CPSD S(�) and transfer function n calculated by core/var to cpsd (A8). onditional case, VAR parameters are ﬁrstly obtained tocovariance sequence �k for the reduced regression e/autocov to var (A6), and are used to transform the nce sequence by core/autocov xform (A11). The condi- ral causalities are then calculated from the transformed nce sequence �†
k as unconditional spectral causalities
(33). There is in fact a simpliﬁcation due to the fact that (A.21)] the power spectrum S†xx(�) for the transformed ble is ﬂat and will thus already have been calculated as nce matrix �†xx. tines: gc/autocov to smvgc, gc/autocov to spwcgc
ation of spectral G-causality
ine gc/smvgc to mvgc implements the spectral MVGC 0) and (34) and the band-limited variants (36) and (37); s 2.6 and 2.7 respectively. Numerical integration is per-
simple trapezoidal rule quadrature. tines: gc/smvgc to mvgc
. Exact solution of a minimal causal VAR(1)
ider the VAR(1) of (38):
+ cYt−1 + εx,t bYt−1 + εy,t
(B.1)
1, |b| < 1 and the residuals εx,t, εy,t are unit-variance d white noise. The coefﬁcients matrix of (B.1) is)
(B.2)
Sxx(z) =
where
h(z) =
�2|1 − for all z
�2(1 +
�0 = 1
�1 = a
ı ≡ (1
�. The CPSD of the joint (Xt, Yt) process (B.1) may thus d from (9) as
1
az|2|1 − bz|2
( |1 − bz|2 + c2 cz(1 − az) cz(1 − az) |1 − az|2
) (B.4)
the complex plane (cf. Barnett and Seth, 2011, Section 4). n, the x-component Sxx(z) of the CPSD may be factorised
the form
= �2h(z)h(z)∗ (B.5)
1 − rz az)(1 − bz) (B.6)
st the residuals variance of the process Xt considered ). Note that this implies that, although the joint process AR(1), the process Xt alone is actually VARMA(2, 1) (cf. ). In order to satisfy (B.5) and (B.6), the unknowns �2
satisfy
= |1 − bz|2 + c2 (B.7) i� on the unit circle in the complex plane, which requires
� ≡ 1 + b2 + c2 (B.8)
(B.10)
note that by assumption �xx = varεx,t ≡ 1, and that �′xx = nite-lag) G-causality in the Y → X direction is 2 = ln
[ 1 2
)] (B.11)
and Seth (2011), Eq. (45)]. late the 1-lag G-causality, we consider the 1-lag reduced
+ ε′t (B.12) eed to solve for the residuals variance � ′2 ≡ var(ε′t) in uares (or ML) sense. The OLS solution is given by the riance (Barnett et al., 2009)
t) − cov(XtXt−1)2var(Xt−1)−1 = �20 − �21
�0 (B.13)
cov(XtXt−k) is the autocovariance sequence for Xt. Now ule-Walker equations (6) for the joint process (Xt, Yt)
some straightforward algebra yields
)(1 − b2) (B.16)
L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68 67
leading to
[ 1 + 2ı + (1 − b2)ı2
Anderson MJ, 2001;43:7
McGraw-H Barnett L, Barr
2013;109: Barnett L, Seth
ance. Phys Barrett AB, M
Granger ca ing propof
Bartels RH, S 1972;15:8
Chen Y, Bress causality a Methods 2
Chen Y, Bressl robiologic evoked res
Cohen MX, networks 2013;23:1
David O, Guill neural driv 2008;6:26
Deshpande G, ﬁndings an Connect 2
Dhamala M, R wavelet tr
Dhamala M, R with nonp
Ding M, Bress cal event-r data prepr 2000;83:3
Ding M, Chen roscience. series anal
Durbin J. The ﬁ Edwards AWF
Press; 199 Efron B. The j
Industrial Engle F, Grang
dimension detection.
Friston KJ. F 2011;1:13
Friston KJ, H 2003;19:1
Gaillard R, Deh intracrania
Gerhard F, Kisp struction o alone. PLo
Geweke J. Measurement of linear dependence and feedback between multiple time series. J Am Stat Assoc 1982;77:304–13.
Geweke J. Measures of conditional linear dependence and feedback between time series. J Am Stat Assoc 1984;79:907–15.
Granger CWJ. Investigating causal relations by econometric models and cross- tral m
CWJ. S iﬁcati
CWJ, ional eth A enous n JD. T rker D lenge age 2 rker onses . Neur
M, onal ;56:2 g Y, T .
en A, K y; 200 . Prin , Sch ;166:
utrino els of y T, G it usi
C, Guo plex i
N. Th iction , Proko
hl H. ag; 20 . Rece
tivaria rie AD ld Scie reas G racting . Hum Bokil H , Vie opy s ://dx.d adi AG aram
M, Wi k like ;17:1
eim A y: Pre Time s
A, P raw-H ausal bridge
DB, W conve s; 199 en PM ysis o e esti ck A, F ger ca ck A, rain u . ck A, c reson ete B,
of ex .
r T. M
ßl H. Dynamics of activity and connectivity in physiological neuronal In: Schuster H, editor. Nonlinear dynamics and neuronal networks.
VCH Publishers Inc.; 1991. p. 281–302. Robinson J. Permutation tests for linear models. Aust New Zeal J Stat 5–88.
The statistical analysis of time series. New York: Wiley; 1971. igital ﬁlters: analysis, design, and applications. New York, NY: ill; 1993. ett AB, Seth AK. Granger causality and transfer entropy are equivalent an variables. Phys Rev Lett 2009;103:238701. somaier T. Transfer entropy as a log-likelihood ratio. Phys Rev Lett 138105.
AK. Behaviour of Granger causality under ﬁltering: theoretical invari- ractical application. J Neurosci Methods 2011;201:404–19. nett L, Seth AK. Multivariate Granger causality and generalized vari-
Rev E 2010;81:41907. urphy M, Bruno MA, Noirhomme Q, Boly M, Laureys S, et al. usality analysis of steady-state electroencephalographic signals dur- ol-induced anaesthesia. PLoS ONE 2012;7:e29072. tewart G. Solution of the equation AX + XB = C. Commun ACM 20–6.
A. Wiener–Granger causality: a well established methodology. Neu- 011;58:323–9. Marse TB, He P, Ding M. Causal measures of structure and plasticity ed and living neural networks. PLoS ONE 2008;3:e3355. ler SL, Ding M. Frequency decomposition of conditional Granger nd application to multivariate neural ﬁeld potential data. J Neurosci 006a;150:228–37. er SL, Knuth KH, Truccolo WA, Ding M. Stochastic modeling of neu- al time series: power, coherence, Granger causality, and separation of ponses from ongoing activity. Chaos 2006;16, 026113–026113. van Gaal S. Dynamic interactions between large-scale brain predict behavioral adaptation after perceptual errors. Cereb Cortex 061–72. key JW. An algorithm for the machine computation of the complex ies. Math Comput 1965;19:297–301. emain I, Saillet S, Reyt S, Deransart C, Segebarth C, et al. Identifying ers with functional MRI: an electrophysiological validation. PLoS Biol 83–97. Hu X. Investigating effective brain connectivity from FMRI data: past d current issues with reference to granger causality analysis. Brain
012;2:235–45. angarajan G, Ding M. Estimating Granger causality from Fourier and ansforms of time series data. Phys Rev Lett 2008a;100:018701. angarajan G, Ding M. Analyzing information ﬂow in brain networks arametric Granger causality. Neuroimage 2008b;41:354–62. ler S, Yang W, Liang H. Short-window spectral analysis of corti- elated potentials by adaptive multivariate autoregressive modeling: ocessing, model validation, and variability assessment. Biol Cybern 5–45. Y, Bressler S. Granger causality: basic theory and application to neu-
In: Schelter S, Winterhalder M, Timmer J, editors. Handbook of time ysis. Wienheim: Wiley; 2006. p. 438–60. tting of time series models. Rev Inst Int Stat 1960;28:233–44. . Likelihood (expanded edition). Baltimore: Johns Hopkins University 2. ackknife, the bootstrap, and other resampling plans. In: Society of and Applied Mathematics CBMS-NSF Monographs; 1982. p. 38. er CWJ. Co-integration and error correction: representation, estima- sting. Econometrica 1987;55:251–76. acoub E. The rapid development of high speed, resolution and preci- RI. Neuroimage 2012. f A, Pirker S, Baumgartner C, Deistler M. Inﬂuence analysis for high- al time series with an application to epileptic seizure onset zone
J Neurosci Methods 2013;214:80–90. ran R, Seth AK. Analysing connectivity with Granger causality and ausal modelling. Curr Opin Neurobiol 2013;23:172–8. unctional and effective connectivity: a review. Brain Connect –36. arrison L, Penny W. Dynamic causal modelling. Neuroimage 273–302. aene S, Adam C, Clémenceau S, Hasboun D, Baulac M, et al. Converging l markers of conscious access. PLoS Biol 2009;7, e61–e61. ersky T, Gutierrez GJ, Marder E, Kramer M, Eden U. Successful recon- f a physiological circuit with known connectivity from spiking activity S Comput Biol 2013;9:e1003138.
spec Granger
spec Granger
estim Schreibe
ethods. Econometrica 1969;37:424–38. ome properties of time series data and their use in econometric model on. J Econometrics 1981;16:121–30. Joyeux R. An introduction to long-memory time series models and differencing. J Time Series Anal 1980;1:15–30. , Kendrick K, Zhou C, Feng J. Partial granger causality: eliminating
inputs and latent variables. J Neurosci Methods 2008;172:79–93. ime series analysis. Princeton, NJ: Princeton University Press; 1994. A, Gonzalez-Castillo J, D’Esposito M, Bandettini PA. The continuing
of understanding and modeling hemodynamic variation in fMRI. Neu- 012. DA, Ollinger JM, D’Esposito M. Variation of BOLD hemodynamic
across subjects and brain regions and their effects on statistical anal- oimage 2004;21:1639–51. Friston KJ, Jan J, Brazdil M, Calhoun VD. Dynamic modeling of responses in fMRI using cubature Kalman ﬁltering. Neuroimage 109–28. amhane AC. Multiple comparison procedures. New York: John Wiley;
arhunen J, Oja E. Independent component analysis. New York: John 1.
cipal component analysis. 2nd ed. New York: Springer-Verlag; 2002. reiber T. Information transfer in continuous processes. Physica D 43–62.
D, Ghosh S, Brown EN. A Granger causality measure for point process ensemble neural spiking activity. PLoS Comput Biol 2011;7:e1001110. utierrez GJ, Marder E. Functional connectivity in a rhythmic inhibitory ng granger causality. Neural Syst Circ 2011:1.
S, Kendrick K, Feng J. Beyond element-wise interactions: identifying nteractions in biological processes. PLoS ONE 2009;4:e6899. e Wiener RMS (root-mean-square) error criterion in ﬁlter design and . J Math Phys 1947;25:261–78. penko M. Differentiating information transfer and causal effect. Eur 10;73:605–15. eng J. Granger causality with signal-dependent noise. Neuroimage 422–9. New introduction to multiple time series analysis. Berlin: Springer- 05. nt trends in multivariate prediction theory. In: Krishnaiah PR, editor. te analysis. New York: Academic Press; 1966. p. 51–382. R, Tsai CL. Regression and time series model selection. Singapore: ntiﬁc Publishing; 1998. , Schoffelen JM, Paterson G, Gross J. Investigating causality between
brain areas with multivariate autoregressive models of MEG sensor Brain Mapp 2013;34:890–913. . Observed brain dynamics. New York: Oxford University Press; 2008. ra A, Lee DTL, Kailath T. Recursive multichannel maximum pectral estimation. IEEE Trans Geosci Electron 1978;16:85–94, oi.org/10.1109/TGE.1978.294569. , Rangarajan G, Jain N, Ding M. Analyzing multiple spike trains with etric Granger causality. J Comput Neurosci 2009;27:55–64. lson MA, Brown EN. Analyzing functional connectivity using a net- lihood model of ensemble neural spiking activity. Neural Comput 927–61. V, Shafer RW, Buck JR. Discrete-time signal processing. 2nd ed New ntice Hall; 1998. eries modeling of neuroscience data. Boca Raton, FL: CRC Press; 2012. illai SU. Random variables and stochastic processes. New York: ill; 2002. ity: models, reasoning and inference. 2nd ed. Princeton, New York:
University Press; 2009. alden AT. Spectral analysis for physical applications: multitaper
ntional univariate techniques. Cambridge, UK: Cambridge University 3. , Abrahamsen TJ, Madsen KH, Hansen LK. Nonlinear denoising and
f neuroimages with kernel principal component analysis and pre- mation. Neuroimage 2012;60:1807–18. ormisano E, Goebel R. Mapping directed inﬂuence over the brain using usality and fMRI. Neuroimage 2005;25:230–42. Formisano E, Goebel R. The identiﬁcation of interacting networks in sing fMRI: model selection, causality and deconvolution. Neuroimage
Seth A, Valdes-Sosa P. Causal time series analysis of functional mag- ance imaging data. J Mach Learn Res 2010;12:65–94.
Rosseel Y. Does partial Granger causality really eliminate the inﬂu- ogenous inputs and latent variables? J Neurosci Methods 2012;206:
tationary random processes. San Francisco: Holden-Day; 1967. ar K, Chen T, Menon V. Multivariate dynamical systems models for
causal interactions in fMRI. Neuroimage 2011;54:807–23. easuring information transfer. Phys Rev Lett 2000;85:461–4.
68 L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68
Seth AK. A MATLAB toolbox for Granger causal connectivity analysis. J Neurosci Methods 2010;186:262–73.
Seth AK, Chorley P, Barnett L. Granger causality analysis of fMRI BOLD signals is invariant to hemodynamic convolution but not downsampling. Neuroimage 2013;65:540–55.
Shkolnisky Y, Sigworth FJ, Singer A. A note on estimating autocovariance from short-time observations; 2008, Unpublished, Available from: http://www. journalogy.org/Publication/5978390/a-note-on-estimating-autocovariance- from-short-time-observations
Silvennoinen A, Teräsvirta T. Multivariate GARCH models. In: Mikosch T, Kreiss JP, Davis RA, Andersen TG, editors. Handbook of ﬁnancial time series. Berlin: Springer-Verlag; 2009. p. 201–29.
Solo V. On causality I: sampling and noise. In: IEEE Proceedings of the 46th Confer- ence on Decision and Control; 2007. p. 3634–9.
Truccolo W, Eden UT, Fellows MR, Donoghue JP, Brown EN. A point process frame- work for relating neural spiking activity to spiking history, neural ensemble, and extrinsic covariate effects. J Neurophysiol 2005;93:1074–89.
Valdes-Sosa PA, Roebroeck A, Daunizeau J, Friston K. Effective connectivity: inﬂu- ence, causality and biophysical modeling. Neuroimage 2011;58:339–61.
Wald A. Tests of statistical hypotheses concerning several parameters when the number of observations is large. Trans Am Math Soc 1943;54:426–82.
Wang X, Chen Y, Ding M. Estimating Granger causality after stimulus onset: a cau- tionary note. Neuroimage 2008;41:767–76.
Welch PD. The use of fast fourier transform for the estimation of power spectra: a method based on time averaging over short, modiﬁed periodograms. IEEE Trans Audio Electroacoust 1967;15:70–3.
Wen X, Yao L, Liu Y, Ding M. Causal interactions in attention networks predict behavioral performance. J Neurosci 2012;32:1284–92.
Whittle P. On the ﬁtting of multivariate autoregressions, and the approximate canonical factorization of a spectral density matrix. Biometrika 1963;50: 129–34.
Wiener N. The theory of prediction. In: Beckenbach EF, editor. Modern mathematics for engineers. New York: McGraw Hill; 1956. p. 165–90.
Wiggins RA, Robinson EA. Recursive solution of the multichannel ﬁltering problem. J Geophys Res 1965;70:1885–91.
Wilks SS. The large-sample distribution of the likelihood ratio for testing composite hypotheses. Ann Math Stat 1938;6:60–2.
Wilson GT. The factorization of matricial spectral densities. SIAM J Appl Math 1972;23:420–6.
Wu GR, Liao W, Stramaglia S, Ding JR, Chen H, Marinazzo D. A blind deconvolution approach to recover effective connectivity brain networks from resting state fMRI data. Med Image Anal 2013;17:365–74.
The MVGC multivariate Granger causality toolbox: A new approach to Granger-causal inference
1 Introduction
2.1 VAR process theory
2.3 Conditional and pairwise-conditional G-causality
2.4 Estimation from time series data
2.5 Statistical inference
2.7 Filter invariance of G-causality
3 MVGC Toolbox design
3.1.2 Computational efficiency and accuracy
3.2 MVGC work-flow
3.4 Debiasing of G-causality magnitudes
4 Application to neuroscience time series data
4.1 Application to surface EEG and MEG data
4.2 Application to intracranial EEG and LFP data
4.3 Application to fMRI BOLD data
4.4 Application to spiking (i.e. point process) data
5 Conclusions
References
The MV bo Grange
Wiener– Geweke, 19 fying “causa Seth, 2011) was operat
∗ Correspon E-mail add
(A.K. Seth).
0165-0270/$ – http://dx.doi.o
dvanced VAR (vector autoregression) theory. arate “full” and “reduced” regressions.
e i n f o
tember 2013 vised form 16 October 2013 ctober 2013
lity ressive modelling alysis
a b s t r a c t
Background: Wiener–Granger causality (“G-causality”) is a statistical notion of causality applicable to time series data, whereby cause precedes, and helps predict, effect. It is deﬁned in both time and frequency domains, and allows for the conditioning out of common causal inﬂuences. Originally developed in the context of econometric theory, it has since achieved broad application in the neurosciences and beyond. Prediction in the G-causality formalism is based on VAR (vector autoregressive) modelling. New method: The MVGC Matlab© Toolbox approach to G-causal inference is based on multiple equiva- lent representations of a VAR model by (i) regression parameters, (ii) the autocovariance sequence and (iii) the cross-power spectral density of the underlying process. It features a variety of algorithms for moving between these representations, enabling selection of the most suitable algorithms with regard to computational efﬁciency and numerical accuracy. Results: In this paper we explain the theoretical basis, computational strategy and application to empirical G-causal inference of the MVGC Toolbox. We also show via numerical simulations the advantages of our Toolbox over previous methods in terms of computational accuracy and statistical inference. Comparison with existing method(s): The standard method of computing G-causality involves estimation of parameters for both a full and a nested (reduced) VAR model. The MVGC approach, by contrast, avoids explicit estimation of the reduced model, thus eliminating a source of estimation error and improving statistical power, and in addition facilitates fast and accurate estimation of the computationally awkward case of conditional G-causality in the frequency domain. Conclusions: The MVGC Toolbox implements a ﬂexible, powerful and efﬁcient approach to G-causal inference.
© 2013 Elsevier B.V. All rights reserved.
ction
Granger causality (G-causality) (Granger, 1969; 82, 1984) is an increasingly popular method for identi- l” connectivity in neural time series data (Bressler and . It can be traced conceptually to Wiener (1956) and ionalised by Granger in terms of linear autoregressive
ding author. Tel.: +44 1273 678101. resses: l.c.barnett@sussex.ac.uk (L. Barnett), a.k.seth@sussex.ac.uk
modelling of stochastic processes (Granger, 1969). G-causality is based on predictability and precedence. Put simply, a variable X is said to G-cause a variable Y if the past of X contains information that helps predict the future of Y over and above information already in the past of Y. Importantly, G-causality is a measure of directed functional connectivity in terms of providing a statistical descrip- tion of observed responses. In contrast, methods for identifying effective connectivity aim to elucidate the “the simplest possible circuit diagram explaining observed responses” (Valdes-Sosa et al., 2011; Aertsen and Preißl, 1991; Friston et al., 2003, 2013).
The MVGC Matlab Toolbox implements numerical routines for calculating multivariate Granger causality (MVGC) from time series
see front matter ©  2013 Elsevier B.V. All rights reserved. rg/10.1016/j.jneumeth.2013.10.018
GC multivariate Granger causality tool r-causal inference
rnett ∗, Anil K. Seth for Consciousness Science and School of Informatics, University of Sussex, Brighton BN1
/ locate / jneumeth
x: A new approach to
K
L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68 51
data, both unconditional and conditional, in the time and frequency domains. It supersedes and extends the GCCA (Granger Causal Con- nectivity Analysis) Toolbox (Seth, 2010). Based on advanced VAR (vector autoregressive) model theory (Hamilton, 1994; Lütkepohl, 2005), it is Improving avoids sep thus elimin computatio multivariat
The MVG ical neurosc applying G- paper (Sect application which has b is always n tions under 3.3). Thus, and straigh standing of as approach facilitate th basis of G- tions), and d toolbox. Al help system purpose—Se rationale an computatio core compu
2. G-causa
Assume cesses (“var Y does not G is independ no informa already con of Y does beyond all i Y G-causes “causality” perhaps mo Sosa et al., 2 for example for the abo this debate exclusively
Testing a statistica seem to in information ever in emp quantities known the mators, com Bossomaier based or pa estimators ling distribu frequently also yield i predictive m
is independent of the past of Y” may be reframed as “the past of Y does not help predict X beyond the degree to which X may be predicted by its own past”.
The most common operationalisation of G-causality, and the one ch the MVGC Toolbox is based, utilises VAR modelling of time data. A multivariate time series u1, u2, . . ., um, where for me t ut is a real-valued n-dimensional (column) vector with nents u1t, u2t, . . ., unt, is considered as a realisation of length discrete-time stationary1 vector stochastic process U1, U2, e “universe of variables”). A pth order vector autoregressive
for the process—a VAR(p)—takes the form2
p
=1 Ak · U t−k + εt (1)
is lued nsio nden ; th eters varia
on s the alues ent t ictab
dete that t was pari mod t ass erve ality bout odel rath s—in s, al ). N ased e, gi ted edict on in
R pr
MVG eory
valid are su v, 19
usal in tances ed”—
ughou ts sca , acco e of th ngth e, and
the de
designed for computational efﬁciency and accuracy. upon standard approaches to G-causal inference, it arate full and reduced regressions (see Section 2), ating a common source of statistical inaccuracy and nal inefﬁciency. It also facilitates estimation of fully e conditional G-causality in the frequency domain. C toolbox has been designed with application to empir- ience data in mind. Several issues commonly faced in causal inference to such data are discussed later in the ion 4). However, the software is not restricted to this
domain. G-causal inference is a very general method een productively applied in many areas, though caution eeded in ensuring that the data satisfy the assump- pinning the method (see Section 2.1 and also Section while the MVGC software is designed to be intuitive tforward to use, to get the most out of it some under-
the theoretical basis of G-causal inference, as well es to its numerical computation, is recommended. To is, this paper presents the conceptual and theoretical causality (substantially extending previous presenta- etails the computational implementation of the MVGC
though not intended as a user guide—the integrated and walk-through demonstration scripts fulﬁll that ction 3 provides a helpful overview of the toolbox d design principles, Section 3.1 explains the MVGC
nal strategy while Appendices A.1 and A.2 detail the tational algorithms.
lity: theory, estimation and inference
two jointly distributed vector-valued stochastic pro- iables”) X = X1, X2, . . ., Y = Y1, Y2, . . .. We say that -cause X if and only if X , conditional on its own past,
ent of the past of Y ; intuitively, past values of Y yield tion about the current value of X beyond information tained in the past of X itself. If, conversely, the past convey information about the future of X above and nformation contained in the past of X then we say that
X . Much has been made of the fact that this notion of does not necessarily tally with more conventional (and re physically intuitive) notions (Pearl, 2009; Valdes- 011; Friston, 2011; Roebroeck et al., 2009, 2010) (note, , that the processes must be strictly non-deterministic ve deﬁnition even to make sense). We do not engage
here; throughout this paper the term “causal” is used in the Wiener–Granger sense just described. for Granger (non-)causality requires establishing—in l sense—a conditional (non-)dependency. This may vite an information-theoretic approach, since mutual
is a natural measure of statistical dependence. How- irical settings a drawback with information-theoretic
is the difﬁculty of estimation in sample and lack of oretical distributions for information-theoretic esti- plicating statistical inference (but see Barnett and
, 2013). An alternative is therefore to invoke a model- rametric approach for which conditional dependency are more efﬁcient and (preferably) have known samp- tions, thus facilitating inference. In fact, G-causality is
identiﬁed with a model-based viewpoint, which may ntuitive predictive interpretations of G-causality; for odels, the statement “ X , conditional on its own past,
on whi series each ti compo m of a . . . (th model
U t = ∑ k
Here p real-va n-dime indepe related param uals co depend model past v repres unpred
The imply cess u In com causal explici the obs G-caus tions a VAR m that a proces as VAR section VAR-b practic is selec ture pr variati
2.1. VA
The cess th
For be squ Rozano
1 G-ca circums “window on.
2 Thro represen matrices the valu cess of le transpos denotes
the model order, which may be inﬁnite. The n × n matrices Ak are the regression coefﬁcients, and the nal stochastic process εt the residuals, which are tly and identically distributed (iid) and serially uncor- at is, they constitute a white noise process. The
of the model are the coefﬁcients Ak and the n × n resid- nce matrix � ≡ cov(εt) which, by stationarity, does not the time t. Interpreted as a predictive model, (1) thus
value of the process at current time t in terms of its at times t − 1, . . ., t − p. The regression coefﬁcients he predictable structure of the data, the residuals the le. rmination of a VAR model (1) should not be taken to the time series data modelled by the stochastic pro-
actually generated by a linear autoregressive scheme. son to effective connectivity techniques like dynamic elling [DCM] (Friston et al., 2003), which make very umptions about the generative mechanism underlying d data (Friston et al., 2013), the VAR models underlying
are “generic”, in the sense that they make no assump- the mechanism that produced the data, beyond that a actually exists. Standard theory (Anderson, 1971) yields er general class of covariance-stationary multivariate cluding many nonlinear processes—may be modelled beit of theoretically inﬁnite order (see also the next ote that this belies a common misapprehension that G-causal analysis is restricted to linear processes. In ven empirical time-series data, a ﬁnite model order p on theoretical principles, sufﬁciently high so as to cap- able variation but not so high as to overﬁt unpredictable
the data (Section 2.4).
ocess theory
C toolbox exploits some advanced aspects of VAR pro- , which we outline here.
G-causality analysis, the VAR coefﬁcients in (1) must mmable and stable (Hamilton, 1994; Lütkepohl, 2005; 67). Square summability requires that
∑p k=1‖Ak‖2 < ∞
ference is not necessarily limited to stationary processes. Under some —notably for multi-trial data or for near-stationary data that may be the stationarity requirement may be relaxed. This is addressed later
t, bold symbols represent vector quantities and normal typeface lar quantities. Capitalised symbols represent random variables or rding to context. Thus we write U t for the random vector representing e process U1, U2, . . . at time t, u1, . . ., um for a realisation of the pro- m, uit for the ith component of ut , etc. A superscript � denotes matrix
a superscript ∗ denotes (complex) matrix conjugate-transpose. |M| terminant of the square matrix M.
52 L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68
which means intuitively that the coefﬁcients do not “blow up” even for inﬁnite model order p (for ﬁnite p square summability is sat- isﬁed trivially). Stability means that the VAR coefﬁcients deﬁne a covariance-stationary process. This is related to the the characteristic polynomial for the coefﬁcients sequence Ak, which is:
ϕA(z) ≡ ∣∣∣∣∣I −
p∑ k=1
Numericall for the �k in for (Ak, �) i
The cros sided) Four
S(�) = ∞∑
k=−∞
ts sim ivate uted
v
(
e prec plex p that (9
able z deﬁned in the complex plane C. By standard the- ohl, 2005) the coefﬁcients Ak deﬁne a stable VAR iff the ic polynomial is invertible on the unit disc |z| ≤ 1 in the ane. Deﬁning the spectral radius of the VAR as
x =0
{|z|−1} (3)
ent condition for stability—i.e. that (1) deﬁne a stationary process—is
(4)
covariance sequence �k for a covariance-stationary (not VAR) stochastic process ut is deﬁned as the sequence rices
, ut−k) k = . . ., −2, −1, 0, 1, 2, . . . (5) y stationarity the �k do not depend on the time t, and k � for all lags k. For a VAR process of the form (1), it may ppendix A.1, (A5)] that in the long run, the autocovari-
nce decays exponentially with lag k, the rate of decay the spectral radius �(A). Note, importantly, that this t if the autocovariance sequence for a process does not )exponentially then the process may not be modelled e Section 3.3). This has some importance for analysis of , which we return to in Section 4.2. R process (1), the autocovariance sequence is related
parameters via the Yule-Walker equations (Anderson,
�k−� + ık0� k = . . ., −2, −1, 0, 1, 2, . . . (6)
y, there are efﬁcient algorithms available for solving (6) terms of (Ak, �) [Appendix A.1, (A5)] and, conversely, n terms of the �k [Appendix A.1, (A6)]. s-power spectral density (CPSD) is deﬁned as the (two- ier transform of the autocovariance sequence3:
�ke −ik� 0 ≤ � ≤ 2� (7)
e n × n Hermitian positive semi-deﬁnite matrices. The rier transform recovers the autocovariance sequence:
�
�
S(�)ei�k d� k = . . ., −2, −1, 0, 1, 2, . . . (8)
y, (7) and (8) may be efﬁciently computed by a (discrete) r Transform (FFT) and Inverse Fast Fourier Transform ctively.
icity, all spectral quantities are deﬁned on the normalised fre- 0 ≤ � ≤ 2�. In an empirical scenario—and indeed in the MVGC n—the natural frequency range of deﬁnition is 0 ≤ � ≤ f where f is the ency in Hertz (Hz). In fact, to take advantage of inherent symmetries, putes all spectral entities only up to the Nyqvist frequency f/2.
While terms that, g and m to (10) a matr
A fu the CPS (Gewe for alm
c−1I ≤
where itive se summ Rozano
Eqs varian mathe a VAR a data. A use of tationa freque
2.2. U
In i is mot distrib
U t = (
Y t
) and th
Ake −ik�
)−1 0 ≤ � ≤ 2� (10)
is no known closed-form solution of (9) for H(�), � in ), a classical result (Masani, 1966; Wilson, 1972) states a CPSD S(�) of a VAR process, a unique solution exists4
computed numerically [Appendix A.1, (A7)]. According AR coefﬁcients Ak may then be recovered from H(�) by ersion and inverse Fourier transform.
r technical condition for valid G-causality analysis is that uniformly bounded away from zero almost everywhere 982, 1984). Explicitly, there must exist a c > 0 such that ll 0 ≤ � ≤ 2�
≤) cI (11)
, B square matrices, ≤A B denotes that B − A is pos- eﬁnite. Importantly, condition (11) guarantees square y of the regression coefﬁcients (Geweke, 1982, 1984; 67). (10) relate the VAR parameters (Ak, �), the autoco-
quence �k and the CPSD S(�). Importantly, these three cal objects specify entirely equivalent representations for ny of them may be estimated from empirical time series
shall see (Section 3), the MVGC toolbox makes extensive equivalences to furnish accurate and efﬁcient compu- hways for the calculation of G-causality in the time and omains.
itional G-causality in the time domain
plest (unconditional, time-domain) form, G-causality d as follows: suppose that U t is split into two jointly
(i.e. inter-dependent) multivariate processes:
(12)
edictive interpretation (cf. Section 2), the G-causality , written FY→X , stands to quantify the “degree to which Y helps predict X , over and above the degree to which
predicted by its own past”. In the VAR formulation, this erationalised as follows: the VAR(p) (1) decomposes as
p
=1
( Axx,k Axy,k
Ayx,k Ayy,k
) ( Xt−k
k=1 Axx,k · Xt−k +
p∑ k=1
Axy,k · Y t−k + εx,t (15)
from which we see that the dependence of X on the past of Y , given its own past, is encapsulated in the coefﬁcients Axy,k; in par- ticular, there is no conditional dependence of X on the past of Y iff Axy,1 = Axy,2 = · · · = Axy,p = 0. This leads to consideration of the reduced (or restricted) regression—as opposed to the full regression (15)—given by omitting the “historic” (past) Y dependency:
Xt = p∑
k=1 A′xx,k · Xt−k + ε′x,t (16)
so that now the reduced sion residua stands to qu resents a “b (16). Maxim natural fram In this fram models of t for its more nically, it is
H0 : Axy,1 = in (15). This the appropr us that the of the form alised varian of the resid domain) G- ratio
FY→X ≡ ln
where �xx = ance matri predictive i of a regress tion error.6
the reductio included in
An impo causality is its interpre for signiﬁca essentially with other G
5 Alternativ or Lagrange m ratio is that is tions (Barrett decomposition tation as a tra et al., 2009; Ba
6 It may see as a measure o ance, frequenc for consistenc priate. See Bar
have a natural interpretation in terms of information-theoretic bits- per-unit-time. This is because, for a large class of joint processes, G-causality and information-theoretic transfer entropy (Schreiber, 2000) are asymptotically equivalent (Barnett and Bossomaier, 2013) [in the Gaussian case the equivalence is exact (Barnett et al., 2009)]. Note that transfer entropy is conceptually isomorphic to G- causality and is often described as a measure of information ﬂow (but see e.g. Lizier and Prokopenko, 2010). Thus, G-causalities may be meaningfully compared, and magnitudes cited, albeit with due caution to s
2.3. Condit
The unco undesirable
pend en sp
is n ) dep ty m y “c e av now in ge onfo een m t al., llust , rec epen
Xt
Y t
=1 A′x
ous t ed in d th , is a
≡ ln
w the FY→ s pre ted b e tha ndit ; tha
that usly th iven i
X is predicted by its own past only. Here A′ xx,k
are regression coefﬁcients and ε′x,t the reduced regres- ls, with covariance matrix �′xx ≡ cov(�′x,t). FY→X , then, antify the degree to which the full regression (15) rep- etter” model of the data than the restricted regression um likelihood (ML) theory (Edwards, 1992) furnishes a ework for the analysis of parametric data modelling.
ework, an appropriate comparative measure for nested he form (15) and (16) is the likelihood ratio statistic or,
convenient statistical properties, its logarithm.5 Tech- a test statistic for the null hypothesis of zero causality
Axy,2 = · · · = Axy,p = 0 (17) motivates the deﬁnition of the G-causality statistic as iate log-likelihood ratio. Now standard ML theory tells likelihood for a realisation of length m of a VAR model
(1) is proportional to |�|−(m−p)/2, where |�|, the gener- ce of the model (Barrett et al., 2010), is the determinant uals covariance matrix. Thus the (unconditional, time- causality from Y to X is deﬁned to be the log-likelihood
|�′xx| |�xx| (18)
cov(εx,t) and �′xx = cov(�′x,t) are the residuals covari- ces of the VAR models (15) and (16) respectively. A nterpretation of (18) is that the generalised variance ion model may be viewed as a measure of model predic- From this perspective, G-causality (18) thus quantiﬁes n in prediction error when the past of the process Y is
the explanatory variables of a VAR model for X . rtant and frequently misunderstood point is that G-
often perceived to lack quantitative meaning beyond tation as a hypothesis-test statistic appropriate only nce testing (Section 2.5); that is, that its magnitude is meaningless, and in particular should not be compared -causality statistics. However, G-causality magnitudes
e test statistics which appear in the G-causality literature are the Wald ultiplier estimators (Edwards, 1992). An advantage of the likelihood has pleasant invariance properties under a broad class of transforma- et al., 2010; Barnett and Seth, 2011) (Section 2.7), a natural spectral
(Geweke, 1982) (Section 2.6) and an information-theoretic interpre- nsfer entropy (Schreiber, 2000; Kaiser and Schreiber, 2002; Barnett rnett and Bossomaier, 2013). m intuitive to use the mean square error trace(�) (the “total error”) f prediction error. However, on the grounds of transformation invari- y decomposition and information-theoretic interpretation, as well as y with the ML formulation, the generalised variance is more appro- rett et al. (2010) for a fuller discussion.
cal) de say, th if there lagged causali nated b they ar on unk it will tially c have b (Guo e
To i known inter-d
U t =
but no effect. Y help predic
Not and co variate
7 Note erroneo bution g (2010).
tatistical signiﬁcance (Section 2.5).
ional and pairwise-conditional G-causality
nditional G-causality statistic introduced above has the characteristic that if there are joint (possibly histori- encies between X and Y and a third set of variables, Z urious causalities may be reported. Thus, for instance, o direct causal inﬂuence Y → X but there are (possibly endencies of X and Y on Z then a spurious Y → X ay be reported. These spurious causalities may be elimi- onditioning out” the common dependencies – provided ailable in the data. If, however, there are dependencies n (exogenous) or unrecorded (latent) variables, then neral be impossible to eliminate entirely their poten- unding effect on causal inference, although attempts ade to mitigate their impact [e.g. “partial” G-causality
2008)7]. rate the conditional case, suppose that the universe U of orded variables splits into three jointly distributed (i.e. dent) multivariate processes
(19)
h to eliminate any joint effect of Z on the inference of lity from Y to X . Again the VAR(p) (1) splits analogously
we may consider the full and reduced regressions
x,k · Xt−k + p∑
k=1 Axy,k · Y t−k +
p∑ k=1
Axz,k · Zt−k + εx,t (20)
x,k · Xt−k + p∑
k=1 A′xz,k · Zt−k + ε′x,t (21)
o (15) and (16), but with the conditioning variables Zt both regressions. The null hypothesis to be tested is still e causality Y → X conditioned on Z, which we write gain as in (18):
|�′xx| |�xx| (22)
inclusion of Z in both regressions accounts for its joint X|Z may thus be read as “the degree to which the past of dict X , over and above the degree to which X is already y its own past and the past of Z”. t in this (and the previous) section, the source, target
ioning variables X , Y , Z may themselves be multi- t is, they may represent groups of variables. It is in this
there are a number of errors in this paper: ﬁrstly, it is stated at partial G-causality may be negative. Secondly, the sampling distri- s incorrect. See Roelstraete and Rosseel (2012) and also Barrett et al.
54 L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68
sense that we use the term “multivariate” G-causality. G-causality is thus able to account for group interactions. This is signiﬁcant, since elements in a multivariate system may function cooperatively or competitively, or interact generally in a more complex fashion than traditional bivariate analysis can accommodate (Ladroue et al., 2009; Barrett et al., 2010).
A case of particular importance is that of pairwise-conditional G-causality. Given a universe U of variables comprising n (known, recorded) jointly distributed univariate processes U1t, . . ., Unt, it is frequently of interest to estimate the G-causalities between pairs of var G-causalitie effects thro it is general causalities
Gi,j(U) ≡ FU
where the s in the multi ity Uj → Ui t are conditio as a weight the (G-)cau
2.4. Estima
So far w stochastic p time series now turn t tion, of FY→ The ﬁrst sta regression ( as the Akai (McQuarrie balance the mined by t ﬁt—perhaps time avoidi
The nex ters which VAR model (Section 1), reduced mo a choice of t to the ML es 1994) and v quently un Whittle, 19 see Append parameters sample esti covariance tors �ˆxx(u) the time do
An impo estimation is not clear
8 Confusing literature to th
9 This sectio 2.2); we need
VAR. In fact condition (11) guarantees that it is.10 However, even if the full VAR (1) is of ﬁnite order, the reduced VAR (16) will gen- erally be of inﬁnite order11 (similar remarks apply to the reduced regression (21)). Since under the ML formalism the full and reduced model orders should be the same, this implies that the appropriate (ﬁnite) empirical model order p should really be estimated for the reduced, rather than the full regression model. Failure to do so has potentially serious implications for G-causal inference (Section 2.5 – see also Section 3.1.2). The MVGC toolbox overcomes this issue in a natural way by only requiring estimation of the full regression
n 3.1).
atisti
nex ted c d, if ood r 1943 e co and mbe
redu esis tor s . Und pto
eter may laced he ca tive
stat xp(F̂ tribu tive on w ity. F ferab ). The F-tes ce G- ely b l asy ual c ques
very ot be cal s ce. F nder , 198 ence itable 3)]. ally, n vie t fam
oted onditi s that ced V ct if der VA
iables Ui, Uj, i /= j. The traditional bivariate pairwise s are the FUj→Ui . Since these are prone to spurious ugh joint dependencies as described above, however, ly preferable to consider rather the pairwise-conditional
j→Ui |U[ij] (23)
ubscript [ij] denotes omission of the ith and jth variables variate universe U . Thus, when considering the causal- he joint dependencies of all remaining known variables ned out.8 The quantities Gij(U), i /= j may be considered ed directed graph, which we shall sometimes refer to as sal graph.
tion from time series data
e have discussed time series data in terms of abstract rocesses; that is, we have assumed that our empirical data behaves like a realisation of some VAR process. We o the crucial issues of estimation, under this assump-
X|Z from9 a numerical time series u = u1, . . ., um. ge is to determine an appropriate model order for the 1). This may be achieved via standard techniques such ke or Bayesian information criteria, or cross-validation
and Tsai, 1998). The idea of model order selection is to number of parameters (in the VAR case this is deter- he maximum lag p) so as to achieve the best model
in a ML or error-minimisation sense—while at the same ng overﬁtting a ﬁnite data sequence. t stage is to obtain estimates of the model parame- maximise the likelihood function for the respective s (equivalently, minimise model error). As mentioned
the MVGC toolbox obviates the need to estimate the del parameters separately from the data. Here there is echniques yielding estimates asymptotically equivalent timate, notably ordinary least squares (OLS) (Hamilton, arious multivariate extensions of Durbin recursion (fre- der the banner of “LWR algorithms”) (Levinson, 1947; 63; Wiggins and Robinson, 1965; Morf et al., 1978) – ix A.1, (A2). Once we have estimates of all relevant
for both the full and reduced models, the G-causality mator F̂Y→X|Z(u) is obtained via (22) with the residuals matrices �xx, �′xx replaced by their respective estima- , �ˆ′xx(u). Another route to estimation of causalities in main is via spectral G-causality (Section 2.6). rtant but rarely considered issue in G-causality model is that, given that the full process ut is a VAR (1), it
that the subprocess Xt will always be a well-deﬁned
ly, the term “multivariate” G-causality is sometimes applied in the is case. n and the following applies equally to the unconditional case (Section simply take Z to be empty; i.e. of dimension 0.
(Sectio
2.5. St
The estima ity, an likelih Wald, both th Y ) = ny the nu nested hypoth estima bution an asym param (which be rep
In t alterna R2-like esis, e d2) dis alterna tributi causal be pre bution to the
Sin positiv centra the act techni
For may n empiri inferen test (A (Efron conﬁd nes su A.1, (A
Fin ered, i accoun
10 As n satisfy c it follow the redu 11 In fa
ﬁnite-or
cal inference
t task is to establish the statistical signiﬁcance of the ausality against the null hypothesis (17) of zero causal- desired, conﬁdence intervals for its magnitude. As a atio test, standard large-sample theory (Wilks, 1938; ) applies to the time-domain G-causality estimator in nditional and unconditional cases. If dim( X) = nx, dim( dim( Z) = nz (with nx + ny + nz = n) then the difference in r of parameters between the full model (20) and the ced model (21) is just d ≡ pnxny. Thus under the null
(17) of zero causality, (m − p) F̂Y→X|Z(u), the G-causality caled by sample size has an asymptotic  2(d) distri- er the alternative hypothesis the scaled estimator has tic noncentral- 2(d ;  ) distribution, with noncentrality = (m − p) FY→X|Z equal to the scaled actual causality , for the purpose of constructing conﬁdence intervals,
by its estimator). se of a univariate causal target [i.e. dim( X) = nx = 1] an asymptotic sampling distribution is available for the istic exp(FY→X|Z) − 1: namely, under the null hypoth- Y→X|Z(u)) − 1 scaled by d2/d1 has an asymptotic F(d1, tion where d1 = pny and d2 = m − p(n + 1), and under the hypothesis an asymptotic noncentral-F(d1, d2 ;  ) dis- here again   is equal to the actual scaled value of the or small samples in particular, the F-distribution may le (it has a fatter tail than the corresponding  2 distri-
MVGC toolbox makes both tests available and defaults t if the target variable is univariate. causality is non-negative, F̂Y→X|Z(u) will generally be iased, and under the alternative hypothesis the non-
mptotic distributions will be more accurate the closer ausality is to zero (Wald, 1943; Geweke, 1982). Some
for dealing with bias are discussed in Section 3.4. small samples the theoretical asymptotic distributions
sufﬁciently accurate and nonparametric data-derived ampling distributions may be preferable for statistical or this purpose the MVGC toolbox supplies permutation son and Robinson, 2001) and non-parametric bootstrap 2) routines for signiﬁcance testing and computation of intervals respectively; the toolbox also features routi-
for simulation of surrogate time series data [Appendix
if multiple causalities are to be simultaneously consid- w of the multiple null hypotheses we should take into ily-wise error rates or false discovery rates (Hochberg
by Geweke (Geweke, 1982), since the full CPSD S(�) is assumed to on (11), so too does the CPSD Sxx(�) of the process X t alone, from which
the coefﬁcients of the autoregression of X t are square summable, and AR is thus well-deﬁned. See Geweke (1982, 1984) for full details. U t is a ﬁnite-order VAR, then a subprocess X t will in general be a RMA process.
L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68 55
and Tamhane, 1987); again, this functionality is available in the MVGC toolbox.
2.6. G-causality in the frequency domain
A powerful feature of G-causality is that it may be decomposed in a natural way by frequency (Geweke, 1982, 1984). The resulting spectral G-causality integrates to the time-domain causality previ- ously introd all frequenc uncondition processes X
S(�) = (
Sx
Sy
of the cros decomposit the CPSD of
Sxx(�) = Hx + H
Following G �xy ≡ 0, wh of variables the simpler
Sxx(�) = Hxx whereby th term. This m causality fro
f Y→X (�) ≡
or, in terms
f Y→X (�) ≡
where the p
�y|x ≡ �yy Geweke the of G-causal
1 2�
∫ 2� 0
f Y
so that tim over all freq
Sample approaches ters Ak, � f model orde Fourier tran ing to (10), from (29). A is then obta Note that n dure (Dham
12 In fact e �yx�
−1 xx Axy(�)|
In practice, ac satisﬁed.
from the data – many standard techniques are available to this end. Then, as mentioned previously (Section 2.1), H(�) and � may be computed numerically and an estimate for the spectral G-causality is again obtained from (28). The MVGC toolbox facilitates both techniques; we recommend the former due to improved numerical accuracy and computational efﬁciency (Section 3.1 and Appendix A.1). As regards statistical inference, in contrast to the time-domain case there are (to our knowledge) no known (asymptotic) distribu-
r the sample distribution of fˆ Y→X (�) (see Geweke, 1984 for discussion on this issue) and nonparametric techniques are ployed for signiﬁcance testing and derivation of conﬁdence ls13.
con nto s d reg
= ∑ k
ne th ntity
lter in
|Sxx(�) − Hxy(�)�yyHxy(�)∗|
|Sxx(�) − Hxy(�)�y|xHxy(�)∗|
) (28)
artial covariance matrix �y|x is deﬁned by
− �yx�−1xx �xy (29) n establishes the fundamental spectral decomposition ity in the unconditional case12
→X (�)d� = FY→X (30)
e domain causality may be thought of as the average uencies of spectral causality. estimation of f Y→X (�) admits at least two possible . A straightforward procedure is to estimate parame- or the VAR(p) (1) as before (after selecting a suitable r). The transfer function may then by calculated by (fast) sform of the estimated regression coefﬁcients accord-
the CPSD from (9) and the partial covariance matrix n estimate for the unconditional spectral G-causality ined by plugging the appropriate estimates into (28). o reduced regression is required. An alternative proce- ala et al., 2008a,b) is to estimate the CPSD S(�) directly
quality in (30) holds strictly when the condition |Ayy(�) − /= 0 is satisﬁed on 0 < � ≤ 2�; otherwise it should be replaced by ≤. cording to Geweke (1982), the equality condition is “almost always”
tions fo a fuller best de interva
The splits i reduce(
Xt
Zt
gij(U; �
xx,k A′
ed, where Y ⊕ Z†t ≡ (
Y t Z†t
) , which expresses the con-
e-domain G-causality FY→X|Z as an unconditional terms of the new variables X†, Y ⊕ Z†. Accordingly, l causality in the unconditional case is deﬁned as
f Y⊕Z†→X† (�) (33)
ctral decomposition
→X|Z(�) d� = FY→X|Z (34)
ns. mate fˆ Y→X|Z(�) in sample, a separate reduced which, as has been pointed out (Chen et al., 2006a),
substantial inaccuracies14—may be avoided as follows: tion of VAR parameters for (1) (or spectral estimation), hown [Appendix A.1, (A11) and (A12)] that the trans- f variables X , Z → X†, Z† deﬁned by (31) induces a
tion of the autocovariance sequence and CPSD which ected computationally. Then the conditional spectral ay be calculated as for the unconditional case via (33).
is known of the sampling distribution of f Y → X|Z(�) nparametric techniques must be used for statistical
he time-domain case, pairwise-conditional spectral may be calculated as
Uj→Ui |U[ij] (�) (35)
variance of G-causality
previously shown that multivariate G-causality, condi- nconditional and in both time and frequency domains,
lished simulation studies suggest that, in fact, the sampling distribu- ) at any ﬁxed frequency � is actually the same as for the time-domain , although we do not yet have a theoretical justiﬁcation for this claim. lso to hold in the conditional case.
decomposition” technique proposed in Chen et al. (2006a) appears, ur unpublished simulations, to yield inaccurate results. In particular, uality (34) generally fails even for simple test data in large samples.
hat the analytic derivation in Chen et al. (2006a) may be ﬂawed.
56 L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68
is theoretically invariant under the application of (almost) arbitrary stable, invertible15 digital ﬁlters (Antoniou, 1993; Barnett and Seth, 2011). Empirically, however, ﬁltering of stationary time series data may severely compromise statistical inference of causalities esti- mated in sample. This is due primarily to an increase in empirical VAR model order following ﬁltering. An important implication is that G-causality restricted to particular frequency bands cannot be measured by ﬁrst ﬁltering the data within the desired bands. While the computed values may change following ﬁltering, this likely reﬂects not ous values a inference. T avoid ﬁlteri spectral cau of interest m causalities and (33) ov (34)] to obt
FY→X (B) ≡
FY→X|Z(B)
where �(B nonparame band-limite
In pract where the d causal anal (Section 3.3 to improve tronically r ﬁnite differ ﬁlters of th (approxima constitutes not be diffe neuroscienc Magnetic R been previo
3. MVGC T
Central (Section 2. ance seque representat these equiv for moving thus furnish of G-causal frequency d mented in t
3.1. Compu
The basi the MVGC t
15 In Barnett a ﬁlter need on imum phase ﬁ for the ﬁltered pointing this o
order) a VAR (1) is ﬁtted to the time series data just once (A2) and all subsequent calculations are based on the estimated model parameters Aˆk, �ˆ. This approach is theoretically consistent with the fundamental assumption of G-causal estimation, that the time series data any of the initial estim spectral est (A2) proves
t; no tive ing e (2 ram Ther d re eters
(20 he d ticul t al., ainin eque
the ce se ll) V raigh tivel se t s rev eters hitt
ore a ilso
. The ant r gh, a d if d usal te w 15).
Redu varia noted l reg ill,
d reg ed a varia ode t, we n H( e VA nsfer r Tran s tha e spe ], wh (A7) he Ak oluti sion. s no e’s a oces
the desired “band-limited” G-causality but rather spuri- rising from inaccuracies in model ﬁtting and statistical he solution proposed in Barnett and Seth (2011) is to ng (of stationary data – see below) altogether. Then if salities are required, values outside the frequency range ay simply be ignored, while appropriate time-domain
may be obtained by averaging spectral causalities (28) er the frequency range B of prior interest [cf. (30) and ain band-limited G-causality:
1 �(B)
∫ B
∫ B
f Y→X|Z(�) d� (37)
) ≡ ∫ B d� is the measure (length) of B. Once again,
tric methods must be used for statistical inference of d G-causality. ice, some ﬁltering may still be appropriate in cases ata are not stationary to begin with, in which case G-
ysis is likely anyway to fail or deliver spurious results ). In these cases ﬁltering may be a valid and useful tool
stationarity; e.g. notch-ﬁltering of line-noise in elec- ecorded time series data (Barnett and Seth, 2011), or encing to eliminate drift (Seth, 2010). In the latter case, e form u˜t = ut − ut−1 may be applied iteratively until te) stationarity is achieved; but note that differencing
a non-invertible ﬁlter, so that stationary data should renced. An important application of ﬁlter invariance in e (Section 4) is to G-causal analysis of fMRI (functional
esonance Imaging) data, where serious confounds have usly suspected; this is discussed further in Section 4.3.
oolbox design
to the design of the MVGC toolbox is the equivalence 1) of the VAR parameters (Ak, �), the autocovari- nce �k and the cross-power spectral density S(�) as ions for a VAR process. The MVGC toolbox exploits alences to provide numerically accurate algorithms
ﬂexibly between the alternative VAR representations, ing computationally efﬁcient pathways for calculation ities, conditional and unconditional, in the time and omains. A schematic of computational pathways imple- he toolbox is given in Fig. 1.
tational strategy
c operating principle of estimation of G-causalities via oolbox is that (after determination of a suitable model
and Seth (2011) it is erroneously stated that to guarantee invariance ly be causal—i.e. not contain a pure lag. In fact full invertibility (min- ltering) is required to ensure that the condition (11) is not violated
process. We are grateful to Victor Solo (private communication) for ut.
efﬁcien alterna
Hav the tim VAR pa tively. reduce param in Seth from t be par Chen e for obt ance s or from varian the (fu and st respec is to u that, a param (A8) (W and m (A7) (W below) domin althou mente of G-ca integra (34) (A
3.1.1. autoco
As the ful order w reduce describ autoco ﬁnite m
Firs functio Now th the tra Fourie implie must b on [0, f rithm Then t tral res regres there i Whittl VAR pr
is a realisation of a stationary VAR (1). In principle, equivalent VAR representations might be chosen for ation—e.g. via sample autocovariance (A1 → A6) or
imation (A4 → A7)—but empirically direct estimation to be the most stable, accurate and computationally netheless the MVGC toolbox allows implementation of methods. acquired the VAR model, (conditional) G-causality in 2) and frequency (33) domains involves estimation of eters for the reduced regressions (21) and (31) respec- e is no simple algorithm (to our knowledge) to compute gression VAR parameters directly from full regression . Indeed, the standard approach [e.g. as implemented 10)] is to perform the reduced regressions explicitly ata (A2); however this leads to inaccuracies that may arly serious in the spectral case (see Section 2.6 and 2006a). Fortunately, numerical algorithms are available g reduced regression parameters from the autocovari- nce (A7) via solution of the Yule-Walker equations (6),
CPSD (A7) via spectral factorisation (9); the autoco- quence and CPSD may themselves be computed from AR parameters by “reverse” Yule-Walker solution (A5) tforward spectral calculation (Fourier transform) (A8) y. The recommended pathway for the MVGC Toolbox he autocovariance sequence. This is on the grounds ealed by empirical testing over a broad range of VAR , system size and model orders, Whittle’s algorithm le, 1963) for Yule-Walker solution is faster, more stable ccurate than Wilson’s spectral factorisation algorithm n, 1972); it also has additional useful properties (see
toolbox thus uses the autocovariance sequence as the epresentation for all G-causality routines (A13, A14) gain, the alternative spectral pathways may be imple- esired. Note that a useful “reality check” for the accuracy ity computations is that time-domain causalities should ith reasonable accuracy to their spectral counterparts
ced model order, spectral resolution and nce lags
previously (Section 2.4), even if the model order for ression (20) is ﬁnite (and in practice a ﬁnite full model of course, need to be selected), the model order for the ressions (21) and (31) will be in theory inﬁnite. If, as bove, reduced VAR parameters are calculated from the nce sequence (or CPSD) how do we choose a suitable l order?
note that spectral factorisation (9) speciﬁes the transfer �) at frequencies � for which the CPSD S(�) is speciﬁed. R coefﬁcients Ak are calculated by matrix inversion of
function followed by Fourier transform. Since the Fast sform (FFT) (Cooley and Tukey, 1965) will be used, this
t to calculate VAR coefﬁcients from the CPSD, the CPSD ciﬁed at some set of frequencies �1, . . . �q evenly spaced ere f is the sampling frequency—indeed, Wilson’s algo- assumes this, and approximates the H(�k) accordingly. will naturally be derived for k = 1, . . ., q; i.e. the spec-
on q will, effectively, be the model order for the reduced Unfortunately, for the spectral factorisation approach
obvious way to choose q. Furthermore (in contrast to lgorithm – see below), given a CPSD S(�) for a stable s evaluated, for a given spectral resolution q at evenly
L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68 57
Fig. 1. Schem outlined in Sec
spaced �k, Wilson’s alg afﬂict the a
For the a choice of q Whittle’s Y variance se calculates A VAR proces thus for lar This implie where the tolerance ( sion). Notin radius �—e parameters mentation tolerance, w racy).
Since th sequence (A sequent sp order for th ture of Whi
atic of computational pathways for the MVGC Toolbox. The “inner triangle” (shaded ci tion 2.1. Bold arrows represent recommended (useful and computationally efﬁcient) pat
there is no guarantee that VAR parameters derived by orithm will actually be stable. These problems do not
utocovariance approach, described next. utocovariance route, there is a natural answer for the , based on speciﬁc properties of VAR processes and ule-Walker solution algorithm (A6). Given an autoco- quence �k for k = 0, 1, . . . q lags, Whittle’s algorithm k for k = 1, . . . q. As noted in Section 2.1, for a stationary s the autocovariance sequence �k decays exponentially; ge enough k, �k will become numerically insigniﬁcant. s that we only need calculate the �k for k≤ some q, truncation decision is based on a speciﬁed numerical which might be determined, e.g. by machine preci- g that the exponential decay factor is just the spectral asily calculated from the (assumed known) full VAR —the MVGC Toolbox reverse Yule-Walker (A5) imple- core/var to autocov calculates q so that �q< a given hich defaults to 10−8 (empirically this yields good accu-
e CPSD is calculated by FFT from the autocovariance 9), q also becomes the spectral resolution for any sub-
ectral calculations. Thus a principled value for model e reduced regression is obtained. A further pleasant fea- ttle’s algorithm is that given the ﬁnite autocovariance
sub-sequen of lags q, th algorithm— model orde ria, and the decay, are e Toolbox, th ﬁtted VAR m
3.1.2. Comp The MV
designed w ever possib Matlab’s c appropriate multi-threa Fourier Tran scope of th
16 It is difﬁcu plexity and sca potential para
rcles) represents the equivalence between the VAR representations hways, while blue arrows represent actual MVGC calculation.
ce �k of a stable VAR model up to any given number e derived VAR parameters are—in contrast to Wilson’s guaranteed to be stable. We emphasise that the full r p as derived by standard model order selection crite-
reduced model order q as derived from autocovariance ffectively independent; as implemented by the MVGC e latter depends only on the spectral radius � of the odel.
utational efﬁciency and accuracy GC Matlab implementation (Appendix A) has been ith great regard to efﬁciency. All algorithms are wher- le completely vectorised and designed to exploit
omputational strengths, including the use, where , of key methods which invoke machine-optimised, ded libraries, such as linear solvers (LAPACK) and Fast sform (FFTW).16 Detailed benchmarking is beyond the
is article, but in testing we have not in general found
lt, in lieu of published information, to deduce the computational com- ling of Matlab’s core algorithms, particularly regarding the impact of llelisation.
58 L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68
performance to be a major issue, even with large, highly multivari- ate datasets.
The most performance-critical algorithm in the MVGC work- ﬂow under normal usage scenarios (Section 3.2) is likely to be the Yule-Walker solution algorithm (A6, core/autocov to var). The per- formance of this algorithm depends critically on the number of autocovariance lags deemed necessary for given numerical accu- racy – which, in turn, depends entirely on the spectral radius of the estimated VAR (see previous Section). Thus performance in practice is closely ti is that, if th “near unsta may well b dual-regres see Section autocovaria the acmaxl the acdect effectively note that st In another very long ti more efﬁcie may becom will thus be
Crucially case—multi (Section 2.6 tion is espe where resu inﬂuences b quency ban the data, os prior intere approach is G-causality tistical infer spectral cau 2006a), the duce spurio MVGC sing rectly (and above is ava
Regardin traditional theoreticall adverse eff comparativ traditional
Xt = aXt−1 Yt =
Here the re and unit-va (stability re G-causality analytically dently FX→ F(∞)Y→X , G-ca
17 This is mit covariance de the Ak tend to converges) for
method where inﬁnite lags are assumed for the reduced regression, and for F(1)Y→X , G-causality as approximated by the traditional dual- regression method, with model order 1 (i.e. 1 autoregression lag) for both full and reduced regressions.
We simulated the process (38) 10, 000 times with time series lengths of 100 time steps, for a = 0.8, b = 0.9 and varying causal coef- ﬁcient c, calculating sample distributions for G-causality estimators for both the statistically signiﬁcant causality Y → X and the statisti- cally non-s
2. W y und
sligh ubsta laine ils to
of t sion
sion o e sev d.
VGC
el o imu es u1
’s “we C”, w
causa
ed to the actual dataset being analysed. An implication e spectral radius � is close to 1—i.e. the VAR estimate is ble”—then the single-regression approach of the MVGC e more computationaly intensive than the traditional sion method (“GCCA mode”, in the toolbox parlance –
3.3) and it may be necessary to limit the number of nce lags.17 This may be achieved either explicitly (via ags parameter) or by adjusting the decay tolerance (via ol parameter) in the routine A6, core/autocov to var, trading-off performance for numerical accuracy (but atistical inference may be compromised – see below). usage scenario—smaller spectral radius coupled with me-series lengths—the MVGC approach may actually be nt, since here VAR estimation (A2, core/tsdata to var) e a computational bottleneck, and a single regression
preferable. however, in arguably the most important usage variate conditional G-causality in the frequency domain )—performance is not the issue at stake. This calcula- cially relevant in neuroscience applications (Section 4), lts are required (a) to exclude common cross-variable y conditioning and (b) to be restricted to speciﬁc fre- ds. The traditional approach here has been to pre-ﬁlter tensibly to restrict causal estimates to frequencies of st; however, as already described (Section 2.7), this
fatally ﬂawed, insofar as it not only fails to restrict as desired, but in addition seriously compromises sta- ence. Thus it becomes essential to compute conditional salities where, as has been established (Chen et al.,
traditional dual-regression approach is known to pro- us results, including negative causal estimates. The
le-regression method handles this essential case cor- here too, the performance/accuracy trade-off described ilable). g accuracy, as previously mentioned (Section 2.4) the dual-regression method fails to take account of the y inﬁnite model order of the reduced regression, with ects on statistical inference. We illustrate this with a e analysis of the MVGC single-regression method vs. the dual-regression method for the minimal VAR(1)
+ cYt−1 + εx,t bYt−1 + εy,t
(38)
siduals εx,t, εy,t are normally distributed, uncorrelated riance white noise, a, b represent decay parameters quires that |a| < 1, |b| < 1) and c controls the strength of
from Y → X. G-causalities for (38) may be fully solved [Appendix B; see also Barnett and Seth (2011)]. Evi- Y ≡ 0 and in Appendix B we calculate expressions for usality as approximated by the MVGC single-regression
igated by the observation that model order estimates based on auto- cay for Whittle’s algorithm may be pessimistically large; in general,
become negligibly small (and the residuals covariance matrix � k� number of autocovariance lags.
in Fig. slightl
F̂(1)Y→X itself s be exp sion fa history disper
tributi by the sion in the me
cantly increas consta
Nex measu sidered
X˜t = Y˜t =
where terms ment n the ab lated t time s Result causal estima the nu II [false rates, signiﬁc regres is quit metho
3.2. M
A causal demo/ follow
1. Mod max seri
18 Solo linear G Granger
igniﬁcant (“null”) causality X → Y. Results are displayed e see [Fig. 2(a)] that, due to ﬁnite-sample effects, F̂(∞)Y→X erestimates the true causality F(∞)Y→X on average, while tly underestimates the 1-lag causality F(1)Y→X – which ntially overestimates the true causality. The latter may d by the fact that the model order 1 reduced regres-
take into account the full explanatory power of the he process Xt on itself. We also see [Fig. 2(b)] that the (as measured by standard deviation) of the sample dis-
signiﬁcantly greater for F̂(1)Y→X ; this may be explained itional sampling error incurred by the extra regres- ed. In the non-causal X → Y direction, we see that both Fig. 2(c)] and standard deviation [Fig. 2(d)] are signiﬁ-
ter for F̂(1)X→Y than for F̂ (∞) X→Y ; while the latter decays with
causal strength c, the former remains approximately
investigated the robustness of G-causal inference to nt noise under the two estimation techniques: we con-
process
x,t
y,t (39)
y,t are normally distributed unit-variance white noise rrelated with the Xt, Yt, representing additive measure-
of intensity  . The addition of noise is known to degrade to detect G-causalities (Solo, 2007).18 Again we simu- ocess (39) 10, 000 times with time series lengths of 100
for a = 0.8, b = 0.9, c = 1 and varying noise intensity  . displayed in Fig. 3. Again we see that mean estimated as well as dispersion are higher for the dual-regression han the single-regression estimates, most severely in n-causal) X → Y direction. Here we also calculated Type ative, Fig. 3(c)] and Type I [false positive, Fig. 3(f)] error
the asymptotic null F-distribution (Section 2.5), at a level of  ˛ = 0.05. We see that while the impact of dual n Type II errors is negligible, the impact on Type I errors ere in comparison with the single-regression MVGC
work-ﬂow
al work-ﬂow for calculation of (conditional) G- from empirical time series data, as exempliﬁed in the
demo.m toolbox demonstration script, is illustrated as
rder estimation: For each model order p up to a chosen m, ﬁt a VAR model to the full “universe of data” time , . . ., um (A2) and calculate the likelihood L ∝ |�−(m−p)/2|.
ak linear GC” corresponds to our G-causality, as opposed to his “strong hich corresponds to what has been independently termed “partial lity” (see Section 2.3).
L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68 59
4 G-causality mean (causal)(a)
0.012
0
2. VAR mod paramete that the the VAR stage).
3. Autocov sequence of lags (a the Yule-
4. Time do [possibly (a) Calcu
regre �k (A (6).
(b) Calcu to (22
(c) Test t using ing c conﬁ
5. Frequen f Y → X|Z spectral (a) Trans
autoc calcu
(b) Calcu the tr
MVGC GCCA
0
0.006
0.012
0.018
(d)
(single-regression) vs. GCCA (dual-regression) accuracy for the minimal causal VAR ated over 10,000 runs of 100 time steps each. (a) sample mean signiﬁcant G-causalit G-causality as estimated by the MVGC method, while F(1) plots the theoretical (1- ausality; (c) sample mean null G-causality F̂X→Y ; (d) standard deviation of null G-c
to calculate the chosen model order criterion (AIC or ct the best model order p according to the criterion. el estimation: Estimate the corresponding VAR model rs (Ak, �) for the selected model order (A2) and check
spectral radius (3) is <1 (other statistical tests on parameters and residuals may be performed at this
(d)
(e)
• If
ariance calculation: Calculate the autocovariance �k from the VAR parameters (A5), to a suitable number s described above). This involves “reverse solution” of Walker equations (6). main: For each conditional causality FY→X|Z required
for the entire causal graph (23)]: late the VAR parameters for both the full and reduced ssions (20) and (21) from the autocovariance sequence 6). This involves solution of the Yule-Walker equations
late the time-domain conditional G-causality according ) (A13). he resulting causalities for signiﬁcance at a given level
the analytical sampling distribution (Section 2.5), tak- are to adjust for multiple hypotheses, and construct dence intervals if desired. cy domain: For each conditional spectral causality (�) required [possibly for all pairwise-conditional
causalities]: form the autocovariance sequence for X , Z to the ovariance sequence for X†t , Z
† t (A11); then, as per (33),
late the unconditional spectral causality f Y⊕Z†→X† (�). late the VAR parameters for the full regression (15) from ansformed autocovariance sequence (A6). late the transfer function H(�) by Fourier transforma- f the regression coefﬁcients (10), and then the CPSD ccording to (9) (A8).
we can integra
3.3. Potenti
Users of box (Seth, occasionall satisfactory nings when be because ments and be situation can be usef “GCCA mod traditional culation of Generally, h in GCCA mo with cautio
Likely re colinearity, moving ave discuss eac
(i) Colinea variable time se
1 2 3 4 causal coefficient
MVGC GCCA
1 2 3 4 causal coefficient
G-causality std. dev (null)
MVGC GCCA
), with a = 0.8, b = 0.9 and varying causal coefﬁcient c (x-axis). Sample
X , as estimated by MVGC and GCCA methods. F(∞) plots the theoretical usality as estimated by the GCCA method; (b) standard deviation of ty. See text for details.
late the partial residuals covariance (29) and then the itional spectral causality according to (28) (A14). the spectral causalities for signiﬁcance at a given level rmutation test (Section 2.5), and construct conﬁdence vals, if desired, by non-parametric bootstrap.
time and frequency domain causalities are required,
alternatively calculate the time-domain causalities by ting their spectral counterparts (34) (A15).
al problems and some solutions
the GCCA (Granger Causal Connectivity Analysis) Tool- 2010), which the MVGC Toolbox supersedes, may
y ﬁnd that time series data which appeared to yield results using the GCCA software trigger errors or war-
analysed via the MVGC Toolbox. This will typically the MVGC Toolbox is more stringent in its require-
performs more thorough error-checking. There may s (see below) where the more robust GCCA approach ul. The MVGC toolbox may optionally be deployed in e” (see demo/mvgc demo GCCA), which takes the more approach of separate full and reduced regressions (cal- conditional spectral causalities will not be available). owever, data which yield apparently reasonable results de but fail in standard MVGC mode should be treated n. asons for reported problems with time series data are (i)
(ii) non-stationarity, (iii) long-term memory, (iv) strong rage component and (v) heteroscedasticity. We brieﬂy h in turn.
rity occurs when there are linear relationships between s (i.e. between individual time series) in multivariate ries data. In this case there is an ambiguity in the VAR
60 L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68
1.2
G-causality mean (causal)(a)
0
0.4
0.8
MVGC GCCA
0 1 2 3 4 noise intensity
MVGC GCCA
0
0.2
0.4
0.6
0
(single-regression) vs. GCCA (dual-regression) accuracy for the minimal causal VAR(1) axis). Sample statistics estimated over 10,000 runs of 100 time steps each. (a) Sample m plots the theoretical (inﬁnite lags) G-causality as estimated by the MVGC method, while andard deviation of signiﬁcant G-causality; Type II error rate (proportion of false negativ y; (f) Type I error rate (proportion of false positives). Error rates are calculated using the xt for details.
ntation of the data. Colinearity (or near-colinearity) will ely be detected in the VAR estimation stage (A2) and
d as “rank-deﬁcient” or “ill-conditioned” regressions by /tsdata to var routine; this should be tested for by the ee demo/mvgc demo). One solution is to eliminate lin- endencies, possibly via a Principal Component Analysis olliffe, 2002) or factor model approach (Flamm et al., r a signal separation technique such as Independent ent Analysis (ICA) (Hyvärinen et al., 2001). cipal stationarity check performed by the MVGC Tool- licitly by the routine utils/var specrad or, more usually,
of the standard workﬂow [Section 3.2, step 3] by to autocov) is that the spectral radius of the estimated
model (1) is less than one (4). If this condition is sﬁed then analysis cannot proceed. Note that the rou- e/var to autocov performs exhaustive error checking
produces useful diagnostics: the utility utils/var info all errors, warnings and diagnostics generated by
to autocov (see e.g. demo/mvgc demo). tationary data may be dealt with in several ways. rity can sometimes be achieved by standard tech- uch as de-trending. As mentioned earlier (Section 2.7),
pre-ﬁlte differen underm root pro stationa G-causa Granger directly
An al ping or window implies better) a of wind varying particul trials: th using e assump indepen erative for an e
1 2 3 4 noise intensity
MVGC GCCA
1 2 3 4 noise intensity
G-causality std. dev (null)
MVGC GCCA
Type I error rate
1 2 3 4 noise intensity
MVGC GCCA
with additive noise (39), with a = 0.8, b = 0.9, c = 1 and varying noise ean signiﬁcant G-causality F̂Y→X , as estimated by MVGC and GCCA
F(1) plots the theoretical (1-lag) causality as estimated by the GCCA es); (d) sample mean null G-causality F̂X→Y ; (e) standard deviation of
asymptotic null F-distribution (Section 2.5) at a signiﬁcance level of
ring (e.g. notch ﬁltering of electrical line noise or ﬁnite cing) may also be useful, albeit at the potential cost of ining model ﬁtting and statistical inference. For unit cesses (processes exhibiting random walk-like non- rities) (Hamilton, 1994), a more principled route to l analysis is via co-integration (Granger, 1981; Engle and , 1987). The MVGC Toolbox is not currently able to deal
with co-integrated processes. ternative approach is to divide the data into (overlap-
non-overlapping) windows, on the logic that shorter s are more likely to be approximately stationary. This
a tradeoff between likelihood of stationarity (shorter is nd accuracy of model ﬁt (longer is better). An advantage owing—assuming there is sufﬁcient data—is that time-
G-causality can be analysed (Ding et al., 2000). This is arly useful given a large number of temporally aligned en so-called “vertical regression” can be implemented
xtremely short windows. This method depends on an tion (which requires justiﬁcation) that each trial is an dent realisation of the same underlying stochastic gen-
process; see the demo/mvgc demo nonstationary script xample.
L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68 61
(iii) The spectral radius condition may also fail if the time series, although stationary, has long-term memory—that is the auto- correlation does not decay exponentially. This may arise in particular for iEEG/LFP data (Section 4.2). In this case, the data is funda which m not curr memory sive Fra and Joy difﬁcult mine th checks o CPSD (c manifes
(iv) A probl contain compon core/var it may fa A.1, A5) GCCA m for fMR 2013) (s square s analysis causal m coefﬁcie ical sett order, so This ma compro
In rar Walker model o turn ou sufﬁcien converg and a re ance.
(v) Finally, assume depend is violat heteros ever, w be mod model w likely to ity can may co there is erature) Heteros which a and/or t models 4) the li scedasti
Future i above chall work (Barn equivalence predictive m within a ma
of analytic (asymptotic) sampling distributions for statistical infer- ence.
3.4. Debiasing of G-causality magnitudes
ausa is s cal s tely of bit y (Ba
accu strate o gen ws an nden ate d n be ufﬂe
lica
oug ime s tion t in
is b , 20 2006 d in
toolb tim
plica
EG d h tim . For s to e ned
of w diffe ends is ma ler a , the e-ﬁlt
2.7) g at
evok , non umb
be i (com e ER on—s
indu s “th th th rial E ume n 3.3 oble of a ly de
mentally unsuited to VAR modelling (cf. Section 2.1), ay silently yield spurious results. The MVGC Toolbox is ently able to deal with such data. Models for long-term
processes do exist, e.g. VARFIMA (Vector Autoregres- ctionally Integrated Moving Average) models (Granger eux, 1980), but G-causal inference for such models is
and the theory underdeveloped. Pre-analysis to deter- e presence of long-term memory may be performed via n sample autocovariance (core/var to autocov) and/or ore/var to cpsd), where long-term memory typically ts itself as power-law behaviour. em may arise when the data, although stationary,
a strong (and in particular a “slow”) moving average ent, violating the condition (11). In this case, the routine
to autocov may report warnings or errors (speciﬁcally, il to solve the associated 1-lag problem – see Appendix
and it may become necessary to use the more robust ode described above. This may in particular be the case I BOLD data (Section 4.3). A recent study (Seth et al., ee also Section 4.3) indicates that empirically, even if ummability [or the condition (11)] is violated, G-causal
may still, under some circumstances, yield meaningful agnitudes and directionality. Clearly, however, if the nts of a VAR are not square summable, then in an empir- ing they cannot be well-approximated at ﬁnite model
that the estimated VAR will inevitably be misspeciﬁed. y result in diminished accuracy of causal estimates and mised statistical inference (Section 2.5). e cases it is possible that, as regards the reverse Yule- calculation (A5) (Whittle’s algorithm), the reduced rder q, as determined by autocovariance decay, may t to be too small for the VAR coefﬁcients Ak to decay tly, preventing the residuals covariance matrix � from ing. This is unlikely to occur given a valid VAR model asonably small choice of autocovariance decay toler-
under standard VAR model scenarios, it is usually d that the variance of the residual terms does not
on the actual values of the process. If this condition ed, the process is said to be heteroscedastic. Note that cedasticity does not in itself violate stationarity. How- hile a stationary heteroscedastic process might well ellable in theory as an (inﬁnite order) VAR, such a ill not be parsimonious and statistical inference is
suffer; indeed, it is well-known that heteroscedastic- invalidate standard statistical signiﬁcance tests, and nfound G-causal inference (Luo et al., 2011). While
extensive research (mainly in the econometrics lit- into GARCH (Generalised AutoRegressive Conditional cedastic) models (Silvennoinen and Teräsvirta, 2009), utoregress residuals variances on their own history he history of the process itself, G-causal analysis of such is somewhat fragmented. In the neurosciences (Section terature on detection and functional analysis of hetero- city (Ozaki, 2012) is limited.
terations of the MVGC will address some or all of the enges. These iterations will take advantage of our recent ett and Bossomaier, 2013) establishing a very general
of G-causality and transfer entropy for a broad class of odels (e.g. VARMA, VARFIMA, GARCH, co-integration) ximum likelihood framework, enabling the derivation
G-c mation statisti accura terms entrop ensure As illu ing is t windo indepe surrog can the non-sh
4. App
Alth sis of t applica opmen review and Hu et al., involve MVGC science
4.1. Ap
M/E the hig nature ation i mentio variety can be term tr analys a simp Second mal pr Section ﬁlterin
For (ERPs) large n 3.3 can native averag variati ing the (i) risk ing wi inter-t
Vol (Sectio of a pr cation spatial
lity is by deﬁnition >0 and hence any empirical esti- ubject to bias. This is not important when assessing igniﬁcance but may be important if the objective is to quantify the magnitude of a G-causality interaction in s (leveraging the asymptotic equivalence with transfer rnett et al., 2009; Barnett and Bossomaier, 2013)). To rate magnitude estimation, debiasing is recommended. d in Barrett et al. (2012), a useful approach to debias- erate surrogate distributions by dividing the data into d (many times) randomly rearranging these windows tly for each variable. The mean G-causality across this ata set should give a good estimate of the bias, which
subtracted from the sample estimate obtained from the d data.
tion to neuroscience time series data
h G-causality is an entirely general method for analy- eries data, the MVGC toolbox has been developed with
to neuroscience data in mind. Methodological devel- this application domain is rapidly advancing and a full eyond the present scope (see, for example, Deshpande 12; Friston et al., 2013; Bressler and Seth, 2011; Ding ). This section summarises some of the main issues
application of G-causality (as implemented by the ox) to some of the more common varieties of neuro-
e-series data.
tion to surface EEG and MEG data
ata is well suited for analysis by G-causality in virtue of e resolution, fast sampling, and its typically stochastic
analysis of steady-state M/EEG the primary consider- nsure that the time-series are covariance stationary. As (Section 3.3), non-stationary data can be treated in a ays which may achieve stationarity. First, time-series renced (Section 2.7) which is useful for removing long-
or drifts; however the interpretation of any subsequent y be altered (linear or piecewise de-trending provide nd less problematic alternative that may be effective).
data can be windowed (Section 3.3). Finally, mini- ering may be applied to help achieve stationarity (see , for example by removing electrical line noise by notch 50 (or 60) Hz (Barnett and Seth, 2011). ed data, as exempliﬁed by event-related potentials stationarity is likely to be a common issue. Given a er of trials, “vertical regression” as outlined in Section mplemented using extremely short windows. An alter- plementary) approach is to subtract the ensemble
P from each trial, which—assuming low inter-trial ERP hould result in stationary residual time series reﬂect- ced response (Chen et al., 2006). However, this method rowing out the baby with the bathwater” by dispens- e ERP itself, and (ii) will fail if there is any substantial RP variability, which is common (Wang et al., 2008). conduction in EEG may lead to excessive colinearity ) among EEG time series in sensor space (this is less m for MEG). Some useful approaches here are appli-
surface Laplacian transform, which has the effect of correlating the data (e.g. Cohen and van Gaal, 2013),
62 L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68
use of factor models (Flamm et al., 2013) or projection of sensor- level VAR model coefﬁcients onto the locations of neural sources (Michalareas et al., 2013). The validity of application to source- localised EEG data depends on the method of source-localisation and is beyond the present scope (see Barrett et al., 2012 for an illustrative example). Finally, as emphasised earlier, spectral G- causality of M/EEG data should be measured by the band-limited approach (Section 2.7) and not by preﬁltering into the desired band- pass and then applying time-domain G-causality.
4.2. Applica
Intracran data also pr surface M/E sharing the rate. Howev sometimes variance on non-station neural and and which which are d well as the possibility f tage/referen and drifts, e fully adequ cially long- adapted me these cavea likely to be illuminatin
4.3. Applica
Applicat controversi Valdes-Sosa the hemody and variab mation of u Second, typ sample inte 1 to 3 s, su Inter-region devastating at the neu than the B analysis wo Perhaps su fMRI BOLD properties, This is bec ﬁlter, to w this turns as a delay proof toget lish this re
19 The proof t and the conseq evidence that fully invertible 2004) requires
include detailed population-based spiking neuron generative models coupled to the biomechanically realistic Balloon–Windkessel model of hemodynamic responses, elim- inating concerns that the invariance properties described above depend on responses.
Unfortu together w ence of BOL are combin
eed I da
g ultr ussen sence mend le by ity be tify s in tion ) co oeck riati
stat tivit econ ses ( ity o est b
al. (2 n.
plica
nt pr s the
its o halle r mo ch is duce likel Kim ot su ly le roce en pr g the gad
each , cho mea sky e tion ed in etica
clus
MV es for
dom ses c fere
tion to intracranial EEG and LFP data
ially recorded EEG (iEEG) and local ﬁeld potential (LFP) ovide data well suited for G-causality analysis. Unlike EG this data is usually spatially highly precise while
advantages of high temporal resolution and sampling er, perhaps counter-intuitively, LPF and iEEG data can appear to be “too clean” inasmuch as the stochastic
which VAR modelling depends is overshadowed by ary deﬂections and ﬂuctuations which may have both non-neural (e.g. due to electrode movement) origins, may reﬂect long-term memory effects (Section 2.1) ifﬁcult to accommodate within a VAR framework. As
techniques described above (for M/EEG), one additional or iEEG/LFP is to transform the data using a bipolar mon- ce which may reduce the impact of common sources mphasising the residual stochastic activity. However, a ate treatment of inherently nonstationary (and espe- term memory) data is likely to require substantially thods, e.g. VARFIMA modelling (Section 3.3). Despite ts, G-causality analysis of stationary iEEG/LFP data is
highly informative; see Gaillard et al. (2009) for an g example.
tion to fMRI BOLD data
ion of G-causality to fMRI BOLD data has been highly al for apparently good reasons; e.g. David et al. (2008),
et al. (2011). First, the BOLD signal (as captured by namic response function, HRF) is an indirect, sluggish, le (inter-regionally and inter-subjectively), transfor- nderlying neural activity (Handwerker et al., 2012). ical fMRI protocols involve severe downsampling with rvals (repetition times, TRs) normally ranging from bstantially longer than typical inter-neuron delays. al HRF variation has been argued to be particularly
for G-causality analysis: if X is causally driving Y ral level, but if the BOLD response to X peaks later OLD response to Y, the suspicion is that G-causality uld falsely infer that Y is driving X (David et al., 2008). rprisingly, this is not the case. In fact, G-causality of
data is robust to a wide variety of changes in HRF including notably their time-to-peak (Seth et al., 2013). ause the HRF is effectively a slow, moving average hich G-causality is in principle invariant. (Intuitively, on recognising that a convolution is not the same . We have recently provided a detailed theoretical her with a range of simulations which fully estab- sult19 (Seth et al., 2013). Notably, these simulations
urns on the identiﬁcation of the HRF convolution as an invertible ﬁlter uent ﬁlter-invariance of G-causality (Section 2.7). While there is good the HRF convolution is causal (Handwerker et al., 2004), whether it is
(i.e. minimum-phase) for typical HRF parameters (Handwerker et al., further research.
are ind to fMR portin (Rasm the ab recom possib causal to iden change as reac and (iv (Roebr HRF va tion of connec blind d proces causal may b Seth et domai
4.4. Ap
Poi resent carries main c able fo approa and re imum 2005; does n arguab point p may th treatin (Nedun volve kernel of the Kisper isfy sta is need a theor
5. Con
The routin quency optimi tical in
simpliﬁed VAR-based generative models of neuronal
nately, the severe downsampling imposed by fMRI, ith measurement noise, still undermine G-causal infer- D data in many applications. When confounding HRFs ed with these other factors, false G-causality inferences likely. This precludes naive application of G-causality ta generally. While technological developments sup- a-low TRs (Feinberg and Yacoub, 2012) and de-noising
et al., 2012) promise to alleviate these problems, in of these developments a conservative methodology is ed. Useful strategies include (i) using as short a TR as
compromising on coverage; (ii) examining changes in G- tween experimental conditions, rather than attempting “ground truth” G-causality patterns; (iii) correlating G-causality magnitude with behavioural variables such
times across trials (or trial blocks) (Wen et al., 2012), mputing the so-called “difference of inﬂuence” term
et al., 2005) which may provide some robustness to on. Alternative promising approaches include estima- e-space models which jointly parameterize functional y and hemodynamic responses (Ryali et al., 2011) or volution of the HRF to retrieve the underlying neuronal Havlicek et al., 2011; Wu et al., 2013). In general, G- f fMRI BOLD data should be treated with caution and e interpreted as exploratory (Friston et al., 2013). See 013) for further discussion of this important application
tion to spiking (i.e. point process) data
ocess data obtained from direct neural recordings rep- other end of the spatiotemporal scale from fMRI, but wn challenges with respect to G-causality analysis. The nge is that point process spike train data is not suit- delling by linear VAR models. A theoretically principled
to replace the VAR modelling step with ﬁtting of full d point process models, within a framework of max- ihood estimation (Okatan et al., 2005; Truccolo et al., et al., 2011; Gerhard et al., 2013). The MVGC toolbox pport this in its current version. A simpler (although ss principled) approach is to obtain an estimate of the ss spectrum of spiking neural data; G-causal analysis oceed via spectral factorisation (Section 2.1), effectively
estimated spectrum as if it derived from a VAR process i et al., 2009). A “quick and dirty” alternative is to con- spike train with a Gaussian or half-Gaussian (causal) osing the kernel width by some appropriate function n inter-spike interval – see e.g. Cadotte et al. (2008), t al. (2011). Assuming the resulting time series data sat- arity tests, G-causality can then be applied. Due caution
any interpretation of the results since this, again, is not lly principled solution.
ions
GC toolbox provides a comprehensive set of Matlab implementing G-causality analysis in the time and fre- ains and in both conditional and unconditional cases. It
omputational efﬁciency, numerical accuracy and statis- nce by leveraging multiple equivalent representations
L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68 63
of a VAR model by regression parameters, the autocovariance sequence, and the cross-power spectral density of the underlying process. By this approach, it is able to compute G-causality quan- tities by a “one shot” regression, without requiring separate “reduced” r estimation causality is functionalit mation, and environmen sive docum
As with via the MVG a good und practical co being the ca major chall of the funct its fruitful a systems, bo
Acknowled
We are g ments, and ﬁnancial su to the Dr. M supports th
Software from www for non-pro License (GP www.gnu.o
Appendix A
A.1. Core al
We now MVGC toolb ate time ser m and assum
u ≡ 1 m
m∑ t=1
u
MVG k, �) e asy
ﬁrst s est u1,
rou
ple autocorrelation sequence [cf. (5)] is estimated as
1 k − 1
m∑ t=k+1
ut · ut−k� k = 0, 1, 2, . . . (A.2)
f 1/(m − k − 1) rather than 1/(m − k) is used to obtain an timate. We do not recommend this as a computational nce (auto)covariance estimates can be unacceptably rror in estimation of the mean (A.1). An experimental lementing a potentially more accurate algorithm due ky et al. (Shkolnisky et al., 2008) is included, but we
do not recommend this pathway. Another drawback ifﬁcult to ascertain how many lags k of autocovariance
A2 VA
The ters (A that ar mate.
The square series Aˆ1, . . .
εˆt = ut
L2 (Euc mente overde in the the res sample
�ˆ = m
The algorit extens Whittl (Morf e matric It is wi that no covaria mator) use in BIC (M which for a s than a of time (1978)
Key
A3 VA
The multiv mally d Ap, �). ients, a autom mate s of time
Key
C toolbox supplies two algorithms for ﬁtting parame- for a VAR model (1). Both yield VAR parameter estimates mptotically equivalent to the corresponding ML esti-
method is a standard OLS, which computes a least- imate for the sample estimators Aˆk. Given a sample time . . ., um and estimated regression coefﬁcient matrices he residual errors for the regression are
p
1
Aˆkut−k t = p + 1, . . ., m (A.3)
, the Aˆk are chosen so as to minimise the mean or E2 = (1/(m − p))
∑m t=p+1‖εˆt‖
2 where ‖· ‖ denotes the n) vector norm. In the MVGC toolbox the OLS is imple- the Matlab “/” (mrdivide) operator, which solves the ined linear system
∑p k=1Aˆkut−k = ut , t = p + 1, . . ., m
-squares sense via QR decomposition. An estimate for ls covariance matrix is then obtained as the unbiased ariance of the residual errors [cf. Eq. (A.2)]:
− 1
m∑ t=p+1
εˆt · εˆ�t (A.4)
nd supported method is a variant of the so called “LWR a term generally referring to a class of multivariate to Durbin recursion (Levinson, 1947; Durbin, 1960; 63; Wiggins and Robinson, 1965)—due to Morf et al.
1978). The Morf variant estimates regression coefﬁcient recursively for k = 1, 2, . . . from the time series data ut. acknowledged to be very stable, and has the advantage y VAR coefﬁcients but also estimates �ˆ for the residuals matrix (and hence the VAR maximum likelihood esti- computed recursively. It is thus extremely efﬁcient for hood-based model selection criteria such as the AIC or rrie and Tsai, 1998), in comparison with OLS estimation, s to be recomputed for each model order. In practice,
estimate, the Morf algorithm may be faster or slower ivalent OLS, depending on number of variables, length es and model order. We refer the reader to Morf et al. etails of the algorithm. tines: core/tsdata to var, core/tsdata to infocrit
ulation
tine core/var to tsdata returns simulated multi-trial, e VAR(p) test data u1, . . ., um according to (1) with nor- buted iid residuals εt, for given VAR parameters (A1, . . ., al samples will generally contain non-stationary trans- ay thus be truncated. Truncation may be performed
lly; a sufﬁcient number of initial samples to approxi- narity is calculated according to the estimated number s for autocovariance to decay to near zero (A5). tine: core/var to tsdata
64 L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68
A4 Spectral estimation
Although we do not recommend spectral estimation as a starting point for MVGC estimation (Sections 3.1 and 3.1.1 – although see Dhamala et a useful pre
The rou methods of series data algorithm u 1998) whic modiﬁed pe the resultin sity estimat Toolbox fun parameters uses a mul from the Ch 2008). In ad bandwidth the user. D multi-taper the Matlab
Key rou
A5 Yule-W
Given V Yule-Walke Firstly, we n since (6) ex culated recu then we can
Conside Walker equ that �0 is a
�0 = A�0A�
This is a di 1972) for �0 Matlab Con the form (A the MVGC t iterative sol a standard on a new se
Upt = ApUpt− where
Upt ≡
Ap)k�
le-W
en a 6) m ump uatio
. .
. .
the
. . in sa ) coe eve riab puta e VA are n odel
al., 2008a) examination of cross-power spectra may be liminary step in time series data analysis. tine core/tsdata to cpsd implements two different
estimating the cross-power spectral density from time (further algorithms may be added in future). The ﬁrst ses Welch’s method (Welch, 1967; Oppenheim et al., h splits the data into overlapping “windows”, computes riodograms of the overlapping segments, and averages g periodograms to produce the power spectral den- es. It is implemented via the Matlab Signal Processing ctions pwelch and cpsd. Window length and overlap
may be speciﬁed by the user. The second algorithm ti-taper method (Percival and Walden, 1993) adapted ronux neural data analysis package (Mitra and Bokil, dition to window length and overlap parameters, a time parameter and the number of tapers may also be set by iscrete prolate spheroidal (Slepian) sequences for the
method are calculated using the function dpss from Signal Processing Toolbox. tine: core/tsdata to cpsd
alker reverse solution
AR(p) parameters (A1, . . ., Ap, �), we wish to solve the r equations (6) for the autocovariance sequence �k. ote that if the �k are known for k = 0, 1, . . ., p − 1, then
presses �k in terms of �k−1, . . ., �k−p, the �k may be cal- rsively for k ≥ p. Thus if we can solve (6) up to k = p − 1
calculate �k up to arbitrary lags. r now the case p = 1. Setting A ≡ A1, the ﬁrst two Yule- ations are �0 = A�1� + � and �1 = A�0, leading to (note
symmetric matrix)
+ � (A.5) screte-time Lyapunov equation (Bartels and Stewart, , for which efﬁcient numerical solvers are available. The trol System Toolbox function dlyap solves equations of .5); if the Control System Toolbox is not available, then oolbox function utils/dlyap aitr implements an efﬁcient ver. Thus we may calculate �0 for a VAR(1). We now use trick (Lütkepohl, 2005) to express a VAR(p) as a VAR(1) t of variables. Given the VAR(p) (1), we obtain the VAR(1)
1 + ε p t (A.6)
1
p+1
A6 Yu
p t ) =
⎛⎜⎜⎜⎜⎜⎝ � 0 . . . 0
... . . .
...
) =
−1 (A.13)
k = 0 equation then gives � = �0 − Aq )
· ( �1 . . . �q
)� . In fact, if the �k are esti-
mple (cf. A1), then (A.13) is just the OLS solution for fﬁcients (cf. A2). r, the solution (A.13) has the practical disadvantage that les it requires inversion of an nq × nq matrix, which may tionally prohibitive if q is large (cf. A5). Furthermore, if R model is stable and of order p, the Ak computed from ot guaranteed to be stable, even if q = p. In practice, the
order will not in general be known exactly; indeed, in
L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68 65
the MVGC approach, the �k will generally be calculated as in (A5) up to q large enough to ensure negligible autocovariance for k > q.
The MVGC Toolbox function core/autocov to var uses instead an LWR algorithm due to Whittle (Whittle, 1963) to solve (6) recur- sively for V algorithm, true VAR is �) is also s n variables sions, and i higher mod details.
Key rou
A7 VAR sp
Although known clos known that exists. The r due to Wil implementa jan (see also then be rec Fourier tran calculation.
We rem tional path a wide ran efﬁciently a domain (i.e tle’s LWR al
Key rou
A8 VAR sp
A9, A10 Tr and cross-
Calculat the Fourier terms of no FFT of a seq
F[G](�) = ∑ k
From (7) we
S(�) = F[�]
s of x rep , wh
2.1) rou
S
B(� d reg
pri d reg s. Alt
AR parameters (A1, . . ., Aq, �). Importantly, Whittle’s unlike the OLS solution (A.13), guarantees that if the stable of order p, then the calculated model (A1, . . ., Aq, table, even if not of the correct order; i.e. if q /= p. For the algorithm requires 2q separate n × n matrix inver- s thus likely to be computationally tractable up to far el orders. The reader is referred to Whittle (1963) for
tine: core/autocov to var
ectral factorisation
as mentioned previously (Section 2.1) there is no ed-form solution of (9) for H(�), � in terms of S(�), it is
a unique solution to the spectral factorisation problem outine core/cpsd to var deploys an iterative algorithm son (Wilson, 1972) to achieve this numerically. The tion is based on code kindly provided by G. Rangara-
Dhamala et al., 2008a,b). The VAR coefﬁcients Ak may overed from H(�) by a matrix inversion and inverse sform; the utility function utils/trfun2var performs this
ark that this is not a recommended MVGC computa- way, since our tests indicate that, numerically, over ge of scenarios, spectral factorisation may be more nd accurately calculated by transforming to the time . to the autocovariance sequence (A10), and using Whit- gorithm (A6). tine: core/cpsd to var
ectral calculation
tion core/var to cpsd implements the relations (9) and to compute a VAR CPSD S(�) from VAR coefﬁcients (Ak,
ourier transform (FFT) is used to calculate the transfer �) from the VAR coefﬁcients. tine: core/var to cpsd
ansforming between the autocovariance sequence power spectral density
ing the CPSD from the autocovariance sequence entails transform (7), implemented in the toolbox as an FFT. In rmalised frequencies, i.e. with period 2�, the (discrete) uence Gk approximates
∞
=0 Gke
∫ 2� 0
of a re related
) e a reduced regression
Xt−k + X†t (A.19)
noise) residuals X†t . Note that we we may restrict the ression to q lags, since from the Yule-Walker equations e seen that �k = 0 for k > q implies that Bk = 0 for k > q.
the residuals X†t as new variables, the problem is to e autocovariance sequence �†
k and CPSD S†(�) for the
d universe U†t = (
X†t Y t
) . Now since the X†t are residuals
sion they are white noise (i.e. iid and serially uncor- that �†
xx,k = ık0�†
xx , where �†
xx = cov (X†t ). We also have
and from (A.19) it follows that the �†xy and � † yx are
convolving corresponding � terms with B:
�† xx
�xy,k − q∑
yx(�)B(�) ∗ Syy(�)
The rout integrals (3 see Section formed by a
Key rou
Appendix B
We cons
= e−i
with z be calc
�2r = We ma
�2 = 1 2
From ( �2] the
1 − bz|2 + c2 az|2|1 − bz|2
= �2h(z)h(z)∗ (B.5)
ransfer function is of the form
1 − rz az)(1 − bz) (B.6)
st the residuals variance of the process Xt considered ). Note that this implies that, although the joint process AR(1), the process Xt alone is actually VARMA(2, 1) (cf. ). In order to satisfy (B.5) and (B.6), the unknowns �2
satisfy
= |1 − bz|2 + c2 (B.7) i� on the unit circle in the complex plane, which requires
� ≡ 1 + b2 + c2 (B.8)
(B.9)
( � +
+ ab)ı a2
1 + (1 + ab)ı
Anderson TW. Antoniou A. D
McGraw-H Barnett L, Barr
for Gaussi Barnett L, Bos
2013;109: Barnett L, Seth
ance and p Barrett AB, Bar
ance. Phys Barrett AB, M
Granger ca ing propof
Bartels RH, S 1972;15:8
Bressler S, Seth roimage 2
Cadotte AJ, De in simulat
Chen Y, Bress causality a Methods 2
Chen Y, Bressl robiologic evoked res
Cohen MX, networks 2013;23:1
Cooley JW, Tu fourier ser
David O, Guill neural driv 2008;6:26
Deshpande G, ﬁndings an Connect 2
Dhamala M, R wavelet tr
Dhamala M, R with nonp
Ding M, Bress cal event-r data prepr 2000;83:3
Ding M, Chen roscience. series anal
Durbin J. The ﬁ Edwards AWF
Press; 199 Efron B. The j
Industrial Engle F, Grang
tion and te Feinberg DA, Y
sion in fM Flamm C, Grae
dimension detection.
Friston K, Mo dynamic c
Friston KJ. F 2011;1:13
Friston KJ, H 2003;19:1
Gaillard R, Deh intracrania
Gerhard F, Kisp struction o alone. PLo
Geweke J. Measurement of linear dependence and feedback between multiple time series. J Am Stat Assoc 1982;77:304–13.
Geweke J. Measures of conditional linear dependence and feedback between time series. J Am Stat Assoc 1984;79:907–15.
Granger CWJ. Investigating causal relations by econometric models and cross- tral m
CWJ. S iﬁcati
CWJ, ional eth A enous n JD. T rker D lenge age 2 rker onses . Neur
M, onal ;56:2 g Y, T .
en A, K y; 200 . Prin , Sch ;166:
utrino els of y T, G it usi
C, Guo plex i
N. Th iction , Proko
J B 20 e T, F ;57:1
hl H. ag; 20 . Rece
tivaria rie AD ld Scie reas G racting . Hum Bokil H , Vie opy s ://dx.d adi AG aram
M, Wi k like ;17:1
eim A y: Pre Time s
A, P raw-H ausal bridge
DB, W conve s; 199 en PM ysis o e esti ck A, F ger ca ck A, rain u . ck A, c reson ete B,
of ex .
YA. S Supek ating
r T. M
ßl H. Dynamics of activity and connectivity in physiological neuronal In: Schuster H, editor. Nonlinear dynamics and neuronal networks.
VCH Publishers Inc.; 1991. p. 281–302. Robinson J. Permutation tests for linear models. Aust New Zeal J Stat 5–88.
The statistical analysis of time series. New York: Wiley; 1971. igital ﬁlters: analysis, design, and applications. New York, NY: ill; 1993. ett AB, Seth AK. Granger causality and transfer entropy are equivalent an variables. Phys Rev Lett 2009;103:238701. somaier T. Transfer entropy as a log-likelihood ratio. Phys Rev Lett 138105.
AK. Behaviour of Granger causality under ﬁltering: theoretical invari- ractical application. J Neurosci Methods 2011;201:404–19. nett L, Seth AK. Multivariate Granger causality and generalized vari-
Rev E 2010;81:41907. urphy M, Bruno MA, Noirhomme Q, Boly M, Laureys S, et al. usality analysis of steady-state electroencephalographic signals dur- ol-induced anaesthesia. PLoS ONE 2012;7:e29072. tewart G. Solution of the equation AX + XB = C. Commun ACM 20–6.
A. Wiener–Granger causality: a well established methodology. Neu- 011;58:323–9. Marse TB, He P, Ding M. Causal measures of structure and plasticity ed and living neural networks. PLoS ONE 2008;3:e3355. ler SL, Ding M. Frequency decomposition of conditional Granger nd application to multivariate neural ﬁeld potential data. J Neurosci 006a;150:228–37. er SL, Knuth KH, Truccolo WA, Ding M. Stochastic modeling of neu- al time series: power, coherence, Granger causality, and separation of ponses from ongoing activity. Chaos 2006;16, 026113–026113. van Gaal S. Dynamic interactions between large-scale brain predict behavioral adaptation after perceptual errors. Cereb Cortex 061–72. key JW. An algorithm for the machine computation of the complex ies. Math Comput 1965;19:297–301. emain I, Saillet S, Reyt S, Deransart C, Segebarth C, et al. Identifying ers with functional MRI: an electrophysiological validation. PLoS Biol 83–97. Hu X. Investigating effective brain connectivity from FMRI data: past d current issues with reference to granger causality analysis. Brain
012;2:235–45. angarajan G, Ding M. Estimating Granger causality from Fourier and ansforms of time series data. Phys Rev Lett 2008a;100:018701. angarajan G, Ding M. Analyzing information ﬂow in brain networks arametric Granger causality. Neuroimage 2008b;41:354–62. ler S, Yang W, Liang H. Short-window spectral analysis of corti- elated potentials by adaptive multivariate autoregressive modeling: ocessing, model validation, and variability assessment. Biol Cybern 5–45. Y, Bressler S. Granger causality: basic theory and application to neu-
In: Schelter S, Winterhalder M, Timmer J, editors. Handbook of time ysis. Wienheim: Wiley; 2006. p. 438–60. tting of time series models. Rev Inst Int Stat 1960;28:233–44. . Likelihood (expanded edition). Baltimore: Johns Hopkins University 2. ackknife, the bootstrap, and other resampling plans. In: Society of and Applied Mathematics CBMS-NSF Monographs; 1982. p. 38. er CWJ. Co-integration and error correction: representation, estima- sting. Econometrica 1987;55:251–76. acoub E. The rapid development of high speed, resolution and preci- RI. Neuroimage 2012. f A, Pirker S, Baumgartner C, Deistler M. Inﬂuence analysis for high- al time series with an application to epileptic seizure onset zone
J Neurosci Methods 2013;214:80–90. ran R, Seth AK. Analysing connectivity with Granger causality and ausal modelling. Curr Opin Neurobiol 2013;23:172–8. unctional and effective connectivity: a review. Brain Connect –36. arrison L, Penny W. Dynamic causal modelling. Neuroimage 273–302. aene S, Adam C, Clémenceau S, Hasboun D, Baulac M, et al. Converging l markers of conscious access. PLoS Biol 2009;7, e61–e61. ersky T, Gutierrez GJ, Marder E, Kramer M, Eden U. Successful recon- f a physiological circuit with known connectivity from spiking activity S Comput Biol 2013;9:e1003138.
spec Granger
spec Granger
fract Guo S, S
Roelstra ence 73–7
Rozanov Ryali S,
estim Schreibe
ethods. Econometrica 1969;37:424–38. ome properties of time series data and their use in econometric model on. J Econometrics 1981;16:121–30. Joyeux R. An introduction to long-memory time series models and differencing. J Time Series Anal 1980;1:15–30. , Kendrick K, Zhou C, Feng J. Partial granger causality: eliminating
inputs and latent variables. J Neurosci Methods 2008;172:79–93. ime series analysis. Princeton, NJ: Princeton University Press; 1994. A, Gonzalez-Castillo J, D’Esposito M, Bandettini PA. The continuing
of understanding and modeling hemodynamic variation in fMRI. Neu- 012. DA, Ollinger JM, D’Esposito M. Variation of BOLD hemodynamic
across subjects and brain regions and their effects on statistical anal- oimage 2004;21:1639–51. Friston KJ, Jan J, Brazdil M, Calhoun VD. Dynamic modeling of responses in fMRI using cubature Kalman ﬁltering. Neuroimage 109–28. amhane AC. Multiple comparison procedures. New York: John Wiley;
arhunen J, Oja E. Independent component analysis. New York: John 1.
cipal component analysis. 2nd ed. New York: Springer-Verlag; 2002. reiber T. Information transfer in continuous processes. Physica D 43–62.
D, Ghosh S, Brown EN. A Granger causality measure for point process ensemble neural spiking activity. PLoS Comput Biol 2011;7:e1001110. utierrez GJ, Marder E. Functional connectivity in a rhythmic inhibitory ng granger causality. Neural Syst Circ 2011:1.
S, Kendrick K, Feng J. Beyond element-wise interactions: identifying nteractions in biological processes. PLoS ONE 2009;4:e6899. e Wiener RMS (root-mean-square) error criterion in ﬁlter design and . J Math Phys 1947;25:261–78. penko M. Differentiating information transfer and causal effect. Eur 10;73:605–15. eng J. Granger causality with signal-dependent noise. Neuroimage 422–9. New introduction to multiple time series analysis. Berlin: Springer- 05. nt trends in multivariate prediction theory. In: Krishnaiah PR, editor. te analysis. New York: Academic Press; 1966. p. 51–382. R, Tsai CL. Regression and time series model selection. Singapore: ntiﬁc Publishing; 1998. , Schoffelen JM, Paterson G, Gross J. Investigating causality between
brain areas with multivariate autoregressive models of MEG sensor Brain Mapp 2013;34:890–913. . Observed brain dynamics. New York: Oxford University Press; 2008. ra A, Lee DTL, Kailath T. Recursive multichannel maximum pectral estimation. IEEE Trans Geosci Electron 1978;16:85–94, oi.org/10.1109/TGE.1978.294569. , Rangarajan G, Jain N, Ding M. Analyzing multiple spike trains with etric Granger causality. J Comput Neurosci 2009;27:55–64. lson MA, Brown EN. Analyzing functional connectivity using a net- lihood model of ensemble neural spiking activity. Neural Comput 927–61. V, Shafer RW, Buck JR. Discrete-time signal processing. 2nd ed New ntice Hall; 1998. eries modeling of neuroscience data. Boca Raton, FL: CRC Press; 2012. illai SU. Random variables and stochastic processes. New York: ill; 2002. ity: models, reasoning and inference. 2nd ed. Princeton, New York:
University Press; 2009. alden AT. Spectral analysis for physical applications: multitaper
ntional univariate techniques. Cambridge, UK: Cambridge University 3. , Abrahamsen TJ, Madsen KH, Hansen LK. Nonlinear denoising and
f neuroimages with kernel principal component analysis and pre- mation. Neuroimage 2012;60:1807–18. ormisano E, Goebel R. Mapping directed inﬂuence over the brain using usality and fMRI. Neuroimage 2005;25:230–42. Formisano E, Goebel R. The identiﬁcation of interacting networks in sing fMRI: model selection, causality and deconvolution. Neuroimage
Seth A, Valdes-Sosa P. Causal time series analysis of functional mag- ance imaging data. J Mach Learn Res 2010;12:65–94.
Rosseel Y. Does partial Granger causality really eliminate the inﬂu- ogenous inputs and latent variables? J Neurosci Methods 2012;206:
tationary random processes. San Francisco: Holden-Day; 1967. ar K, Chen T, Menon V. Multivariate dynamical systems models for
causal interactions in fMRI. Neuroimage 2011;54:807–23. easuring information transfer. Phys Rev Lett 2000;85:461–4.
68 L. Barnett, A.K. Seth / Journal of Neuroscience Methods 223 (2014) 50– 68
Seth AK. A MATLAB toolbox for Granger causal connectivity analysis. J Neurosci Methods 2010;186:262–73.
Seth AK, Chorley P, Barnett L. Granger causality analysis of fMRI BOLD signals is invariant to hemodynamic convolution but not downsampling. Neuroimage 2013;65:540–55.
Shkolnisky Y, Sigworth FJ, Singer A. A note on estimating autocovariance from short-time observations; 2008, Unpublished, Available from: http://www. journalogy.org/Publication/5978390/a-note-on-estimating-autocovariance- from-short-time-observations
Silvennoinen A, Teräsvirta T. Multivariate GARCH models. In: Mikosch T, Kreiss JP, Davis RA, Andersen TG, editors. Handbook of ﬁnancial time series. Berlin: Springer-Verlag; 2009. p. 201–29.
Solo V. On causality I: sampling and noise. In: IEEE Proceedings of the 46th Confer- ence on Decision and Control; 2007. p. 3634–9.
Truccolo W, Eden UT, Fellows MR, Donoghue JP, Brown EN. A point process frame- work for relating neural spiking activity to spiking history, neural ensemble, and extrinsic covariate effects. J Neurophysiol 2005;93:1074–89.
Valdes-Sosa PA, Roebroeck A, Daunizeau J, Friston K. Effective connectivity: inﬂu- ence, causality and biophysical modeling. Neuroimage 2011;58:339–61.
Wald A. Tests of statistical hypotheses concerning several parameters when the number of observations is large. Trans Am Math Soc 1943;54:426–82.
Wang X, Chen Y, Ding M. Estimating Granger causality after stimulus onset: a cau- tionary note. Neuroimage 2008;41:767–76.
Welch PD. The use of fast fourier transform for the estimation of power spectra: a method based on time averaging over short, modiﬁed periodograms. IEEE Trans Audio Electroacoust 1967;15:70–3.
Wen X, Yao L, Liu Y, Ding M. Causal interactions in attention networks predict behavioral performance. J Neurosci 2012;32:1284–92.
Whittle P. On the ﬁtting of multivariate autoregressions, and the approximate canonical factorization of a spectral density matrix. Biometrika 1963;50: 129–34.
Wiener N. The theory of prediction. In: Beckenbach EF, editor. Modern mathematics for engineers. New York: McGraw Hill; 1956. p. 165–90.
Wiggins RA, Robinson EA. Recursive solution of the multichannel ﬁltering problem. J Geophys Res 1965;70:1885–91.
Wilks SS. The large-sample distribution of the likelihood ratio for testing composite hypotheses. Ann Math Stat 1938;6:60–2.
Wilson GT. The factorization of matricial spectral densities. SIAM J Appl Math 1972;23:420–6.
Wu GR, Liao W, Stramaglia S, Ding JR, Chen H, Marinazzo D. A blind deconvolution approach to recover effective connectivity brain networks from resting state fMRI data. Med Image Anal 2013;17:365–74.
The MVGC multivariate Granger causality toolbox: A new approach to Granger-causal inference
1 Introduction
2 G-causality: theory, estimation and inference
2.1 VAR process theory
2.2 Unconditional G-causality in the time domain
2.3 Conditional and pairwise-conditional G-causality
2.4 Estimation from time series data
2.5 Statistical inference
2.6 G-causality in the frequency domain
2.7 Filter invariance of G-causality
3 MVGC Toolbox design
3.1 Computational strategy
3.3 Potential problems and some solutions
3.4 Debiasing of G-causality magnitudes
4 Application to neuroscience time series data
4.1 Application to surface EEG and MEG data
4.2 Application to intracranial EEG and LFP data
4.3 Application to fMRI BOLD data
4.4 Application to spiking (i.e. point process) data
5 Conclusions
Acknowledgements
